<!DOCTYPE HTML><html lang="zh-ch"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><title>News:1 paper accepted by ECCV2018 | Xu Tang&#39;s Homepages</title><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no"><meta name="author" content="Xu Tang&#39;s Homepages"><meta name="description" content="Pyramidbox 人脸检测Table of Contents 简介 数据准备 模型训练 模型评估 模型发布  简介人脸检测是经典的计算机视觉任务，非受控场景中的小脸、模糊和遮挡的人脸检测是这个方向上最有挑战的问题。PyramidBox 是一种基于SSD的单阶段人脸检测器，它利用上下文信息解决困难人脸的检测问题。如下图所示，PyramidBox在六个尺度的特征图上进行不同层级的预测。该工作主要包"><meta name="keywords" content="Publications"><meta property="og:type" content="article"><meta property="og:title" content="News:1 paper accepted by ECCV2018"><meta property="og:url" content="http://tangxuvis.com/2018/09/04/eccv2018_pyramidbox/index.html"><meta property="og:site_name" content="Xu Tang&#39;s Homepages"><meta property="og:description" content="Pyramidbox 人脸检测Table of Contents 简介 数据准备 模型训练 模型评估 模型发布  简介人脸检测是经典的计算机视觉任务，非受控场景中的小脸、模糊和遮挡的人脸检测是这个方向上最有挑战的问题。PyramidBox 是一种基于SSD的单阶段人脸检测器，它利用上下文信息解决困难人脸的检测问题。如下图所示，PyramidBox在六个尺度的特征图上进行不同层级的预测。该工作主要包"><meta property="og:locale" content="zh-ch"><meta property="og:image" content="http://tangxuvis.com/imgs/eccv2018_pyramidbox/architecture_of_pyramidbox.jpg"><meta property="og:image" content="http://tangxuvis.com/imgs/eccv2018_pyramidbox/demo_img.jpg"><meta property="og:image" content="http://tangxuvis.com/imgs/eccv2018_pyramidbox/0_Parade_marchingband_1_356.jpg"><meta property="og:image" content="http://tangxuvis.com/imgs/eccv2018_pyramidbox/28_Sports_Fan_Sports_Fan_28_770.jpg"><meta property="og:image" content="http://tangxuvis.com/imgs/eccv2018_pyramidbox/4_Dancing_Dancing_4_194.jpg"><meta property="og:image" content="http://tangxuvis.com/imgs/eccv2018_pyramidbox/12_Group_Group_12_Group_Group_12_935.jpg"><meta property="og:image" content="http://tangxuvis.com/imgs/eccv2018_pyramidbox/wider_pr_cruve_int_easy_val.jpg"><meta property="og:image" content="http://tangxuvis.com/imgs/eccv2018_pyramidbox/wider_pr_cruve_int_medium_val.jpg"><meta property="og:image" content="http://tangxuvis.com/imgs/eccv2018_pyramidbox/wider_pr_cruve_int_hard_val.jpg"><meta property="og:updated_time" content="2019-10-05T18:25:25.220Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="News:1 paper accepted by ECCV2018"><meta name="twitter:description" content="Pyramidbox 人脸检测Table of Contents 简介 数据准备 模型训练 模型评估 模型发布  简介人脸检测是经典的计算机视觉任务，非受控场景中的小脸、模糊和遮挡的人脸检测是这个方向上最有挑战的问题。PyramidBox 是一种基于SSD的单阶段人脸检测器，它利用上下文信息解决困难人脸的检测问题。如下图所示，PyramidBox在六个尺度的特征图上进行不同层级的预测。该工作主要包"><meta name="twitter:image" content="http://tangxuvis.com/imgs/eccv2018_pyramidbox/architecture_of_pyramidbox.jpg"><link rel="alternative" href="/atom.xml" title="Xu Tang&#39;s Homepages" type="application/atom+xml"><link rel="icon" href="/img/favicon-32x32.ico"><link rel="apple-touch-icon" href="/img/jacman.jpg"><link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg"><link rel="stylesheet" href="/css/style.css"></head></html><body><header><div><div id="imglogo"><a href="/"><img src="/img/logo.png" alt="Xu Tang&#39;s Homepages" title="Xu Tang&#39;s Homepages"></a></div><div id="textlogo"><h1 class="site-name"><a href="/" title="Xu Tang&#39;s Homepages">Xu Tang&#39;s Homepages</a></h1><h2 class="blog-motto">Computer Vision &amp; Deep Learning &amp; Face Detection &amp; Object detection &amp; Object Tracking</h2></div><div class="navbar"><a class="navbutton navmobile" href="#" title="Menu"></a></div><nav class="animated"><ul><ul><li><a href="/">Home</a></li><li><a href="/archives">Archives</a></li><li><a href="/tags">Tags</a></li><li><a href="/resume">Resume</a></li><li><a href="/recruit">Recruit</a></li><li><a href="/about">About</a></li><li><form class="search" action="//google.com/search" method="get" accept-charset="utf-8"><label>Search</label> <input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search"> <input type="hidden" name="q" value="site:tangxuvis.com"></form></li></ul></ul></nav></div></header><div id="container"><div id="main" class="post" itemscope itemprop="blogPost"><article itemprop="articleBody"><header class="article-info clearfix"><h1 itemprop="name"><a href="/2018/09/04/eccv2018_pyramidbox/" title="News:1 paper accepted by ECCV2018" itemprop="url">News:1 paper accepted by ECCV2018</a></h1><p class="article-author">By <a href="/about" title="Xu Tang&#39;s Homepages" target="_blank" itemprop="author">Xu Tang&#39;s Homepages</a></p><p class="article-time"><time datetime="2018-09-04T15:36:59.000Z" itemprop="datePublished">Published 2018-09-04</time></p></header><div class="article-content"><div id="toc" class="toc-article"><strong class="toc-title">Contents</strong><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Pyramidbox-人脸检测"><span class="toc-number">1.</span> <span class="toc-text">Pyramidbox 人脸检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Table-of-Contents"><span class="toc-number">2.</span> <span class="toc-text">Table of Contents</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#简介"><span class="toc-number">2.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据准备"><span class="toc-number">2.2.</span> <span class="toc-text">数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型训练"><span class="toc-number">2.3.</span> <span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#下载预训练模型"><span class="toc-number">2.3.1.</span> <span class="toc-text">下载预训练模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#开始训练"><span class="toc-number">2.3.2.</span> <span class="toc-text">开始训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型评估"><span class="toc-number">2.4.</span> <span class="toc-text">模型评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型预测以及可视化"><span class="toc-number">2.5.</span> <span class="toc-text">模型预测以及可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型发布"><span class="toc-number">2.6.</span> <span class="toc-text">模型发布</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#性能曲线"><span class="toc-number">2.6.1.</span> <span class="toc-text">性能曲线</span></a></li></ol></li></ol></li></ol></div><h2 id="Pyramidbox-人脸检测"><a href="#Pyramidbox-人脸检测" class="headerlink" title="Pyramidbox 人脸检测"></a>Pyramidbox 人脸检测</h2><h2 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h2><ul><li><a href="#简介">简介</a></li><li><a href="#数据准备">数据准备</a></li><li><a href="#模型训练">模型训练</a></li><li><a href="#模型评估">模型评估</a></li><li><a href="#模型发布">模型发布</a></li></ul><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>人脸检测是经典的计算机视觉任务，非受控场景中的小脸、模糊和遮挡的人脸检测是这个方向上最有挑战的问题。<a href="https://arxiv.org/pdf/1803.07737.pdf" target="_blank" rel="noopener">PyramidBox</a> 是一种基于SSD的单阶段人脸检测器，它利用上下文信息解决困难人脸的检测问题。如下图所示，PyramidBox在六个尺度的特征图上进行不同层级的预测。该工作主要包括以下模块：LFPN、Pyramid Anchors、CPM、Data-anchor-sampling。具体可以参考该方法对应的论文 <a href="https://arxiv.org/pdf/1803.07737.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1803.07737.pdf</a> ，下面进行简要的介绍。</p><p align="center"><img src="/imgs/eccv2018_pyramidbox/architecture_of_pyramidbox.jpg" height="400" width="600" hspace="10"><br>Pyramidbox 人脸检测模型</p><a id="more"></a><p><strong>LFPN</strong>: LFPN全称Low-level Feature Pyramid Networks, 在检测任务中，LFPN可以充分结合高层次的包含更多上下文的特征和低层次的包含更多纹理的特征。高层级特征被用于检测尺寸较大的人脸，而低层级特征被用于检测尺寸较小的人脸。为了将高层级特征整合到高分辨率的低层级特征上，我们从中间层开始做自上而下的融合，构建Low-level FPN。</p><p><strong>Pyramid Anchors</strong>: 该算法使用半监督解决方案来生成与人脸检测相关的具有语义的近似标签，提出基于anchor的语境辅助方法，它引入有监督的信息来学习较小的、模糊的和部分遮挡的人脸的语境特征。使用者可以根据标注的人脸标签，按照一定的比例扩充，得到头部的标签（上下左右各扩充1/2）和人体的标签（可自定义扩充比例）。</p><p><strong>CPM</strong>: CPM全称Context-sensitive Predict Module, 本方法设计了一种上下文敏感结构(CPM)来提高预测网络的表达能力。</p><p><strong>Data-anchor-sampling</strong>: 设计了一种新的采样方法，称作Data-anchor-sampling，该方法可以增加训练样本在不同尺度上的多样性。该方法改变训练样本的分布，重点关注较小的人脸。</p><p>Pyramidbox模型可以在以下示例图片上展示鲁棒的检测性能，该图有一千张人脸，该模型检测出其中的880张人脸。</p><p align="center"><img src="/imgs/eccv2018_pyramidbox/demo_img.jpg" height="300" width="500" hspace="10"><br>Pyramidbox 人脸检测性能展示</p><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>本教程使用 <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" target="_blank" rel="noopener">WIDER FACE 数据集</a> 来进行模型的训练测试工作，官网给出了详尽的数据介绍。</p><p>WIDER FACE数据集包含32,203张图片，其中包含393,703个人脸，数据集的人脸在尺度、姿态、遮挡方面有较大的差异性。另外WIDER FACE数据集是基于61个场景归类的，然后针对每个场景，随机的挑选40%作为训练集，10%作为验证集，50%作为测试集。</p><p>首先，从官网训练集和验证集，放在<code>data</code>目录，官网提供了谷歌云和百度云下载地址，请依据情况自行下载。并下载训练集和验证集的标注信息:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./data/download.sh</span><br></pre></td></tr></table></figure><p>准备好数据之后，<code>data</code>目录如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">|-- download.sh</span><br><span class="line">|-- wider_face_split</span><br><span class="line">|   |-- readme.txt</span><br><span class="line">|   |-- wider_face_train_bbx_gt.txt</span><br><span class="line">|   |-- wider_face_val_bbx_gt.txt</span><br><span class="line">|   `-- ...</span><br><span class="line">|-- WIDER_train</span><br><span class="line">|   `-- images</span><br><span class="line">|       |-- 0--Parade</span><br><span class="line">|       ...</span><br><span class="line">|       `-- 9--Press_Conference</span><br><span class="line">`-- WIDER_val</span><br><span class="line">    `-- images</span><br><span class="line">        |-- 0--Parade</span><br><span class="line">        ...</span><br><span class="line">        `-- 9--Press_Conference</span><br></pre></td></tr></table></figure><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><h4 id="下载预训练模型"><a href="#下载预训练模型" class="headerlink" title="下载预训练模型"></a>下载预训练模型</h4><p>我们提供了预训练模型，模型是基于VGGNet的主干网络，使用如下命令下载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://paddlemodels.bj.bcebos.com/vgg_ilsvrc_16_fc_reduced.tar.gz</span><br><span class="line">tar -xf vgg_ilsvrc_16_fc_reduced.tar.gz &amp;&amp; rm -f vgg_ilsvrc_16_fc_reduced.tar.gz</span><br></pre></td></tr></table></figure><p>声明：该预训练模型转换自<a href="http://cs.unc.edu/~wliu/projects/ParseNet/VGG_ILSVRC_16_layers_fc_reduced.caffemodel" target="_blank" rel="noopener">Caffe</a>。不久，我们会发布自己预训练的模型。</p><h4 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h4><p><code>train.py</code> 是训练模块的主要执行程序，调用示例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -u train.py --batch_size=16 --pretrained_model=vgg_ilsvrc_16_fc_reduced</span><br></pre></td></tr></table></figure><ul><li><p>可以通过设置 <code>export CUDA_VISIBLE_DEVICES=0,1,2,3</code> 指定想要使用的GPU数量，<code>batch_size</code>默认设置为12或16。</p></li><li><p>更多的可选参数见:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py --<span class="built_in">help</span></span><br></pre></td></tr></table></figure></li><li><p>模型训练150轮以上可以收敛。用Nvidia Tesla P40 GPU 4卡并行，<code>batch_size=16</code>的配置，每轮训练大约40分钟，总共训练时长大约100小时</p></li></ul><p>模型训练所采用的数据增强：</p><p><strong>数据增强</strong>：数据的读取行为定义在 <code>reader.py</code> 中，所有的图片都会被缩放到640x640。在训练时还会对图片进行数据增强，包括随机扰动、翻转、裁剪等，和<a href="https://github.com/PaddlePaddle/models/blob/develop/fluid/PaddleCV/object_detection/README.md" target="_blank" rel="noopener">物体检测SSD算法</a>中数据增强类似，除此之外，增加了上面提到的Data-anchor-sampling:</p><p><strong>尺度变换(Data-anchor-sampling)</strong>：随机将图片尺度变换到一定范围的尺度，大大增强人脸的尺度变化。具体操作为根据随机选择的人脸高(height)和宽(width)，得到$v=\sqrt{width * height}$，判断$v$的值位于缩放区间$[16，32，64，128，256，512]$中的的哪一个。假设$v=45$，则选定$32&lt;v&lt;64$，以均匀分布的概率选取$[16，32，64]$中的任意一个值。若选中$64$，则该人脸的缩放区间在 $[64 / 2，min(v * 2, 64 * 2)]$中选定。</p><p><strong>注意</strong>：</p><ul><li>本次开源模型中CPM模块与论文中有些许不同，相比论文中CPM模块训练和测试速度更快。</li><li>Pyramid Anchors模块的body部分可以针对不同情况，进行相应的长宽设置来调参。同时face、head、body部分的loss对应的系数也可以通过调参优化。</li></ul><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>验证集的评估需要两个步骤：先预测出验证集的检测框和置信度，再利用WIDER FACE官方提供的评估脚本得到评估结果。</p><ul><li><p>预测检测结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -u widerface_eval.py --model_dir=output/159 --pred_dir=pred</span><br></pre></td></tr></table></figure><p>更多的可选参数:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -u widerface_eval.py --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><p><strong>注意</strong>： <code>widerface_eval.py</code>中<code>multi_scale_test_pyramid</code>可用可不用，由于Data-anchor-sampling的作用，更加密集的anchors对性能有更大的提升。</p></li><li><p>评估AP指标</p><p>下载官方评估脚本，评估average precision(AP)指标：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/eval_script/eval_tools.zip</span><br><span class="line">unzip eval_tools.zip &amp;&amp; rm -f eval_tools.zip</span><br></pre></td></tr></table></figure><p>修改<code>eval_tools/wider_eval.m</code>中检测结果保存的路径和将要画出的曲线名称：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 此处修改存放结果的文件夹名字</span><br><span class="line">pred_dir = &apos;./pred&apos;;  </span><br><span class="line"># 此处修改将要画出的曲线名称</span><br><span class="line">legend_name = &apos;Fluid-PyramidBox&apos;;</span><br></pre></td></tr></table></figure><p><code>wider_eval.m</code>是评估模块的主要执行程序，命令行式的运行命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matlab -nodesktop -nosplash -nojvm -r <span class="string">"run wider_eval.m;quit;"</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="模型预测以及可视化"><a href="#模型预测以及可视化" class="headerlink" title="模型预测以及可视化"></a>模型预测以及可视化</h3><p><code>widerface_eval.py</code>也可以用来做预测及可视化，调用示例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python widerface_eval.py --infer=True --confs_threshold=0.15</span><br><span class="line"> --model_dir=output/159/ --image_path=data/WIDER_train/images/0--Parade/0_Parade_marchingband_1_219.jpg</span><br></pre></td></tr></table></figure><p>下图可视化了模型的预测结果：</p><p align="center"><img src="/imgs/eccv2018_pyramidbox/0_Parade_marchingband_1_356.jpg" height="300" width="300" hspace="10"> <img src="/imgs/eccv2018_pyramidbox/28_Sports_Fan_Sports_Fan_28_770.jpg" height="300" width="300" hspace="10"> <img src="/imgs/eccv2018_pyramidbox/4_Dancing_Dancing_4_194.jpg" height="300" width="300" hspace="10"> <img src="/imgs/eccv2018_pyramidbox/12_Group_Group_12_Group_Group_12_935.jpg" height="300" width="300" hspace="10"><br>Pyramidbox 预测可视化</p><h3 id="模型发布"><a href="#模型发布" class="headerlink" title="模型发布"></a>模型发布</h3><table><thead><tr><th align="center">模型</th><th align="center">预训练模型</th><th align="center">训练数据</th><th align="center">测试数据</th><th align="center">mAP</th></tr></thead><tbody><tr><td align="center"><a href="http://paddlemodels.bj.bcebos.com/PyramidBox_WiderFace.tar.gz" target="_blank" rel="noopener">Pyramidbox-v1-SSD 640x640</a></td><td align="center"><a href="http://paddlemodels.bj.bcebos.com/vgg_ilsvrc_16_fc_reduced.tar.gz" target="_blank" rel="noopener">VGGNet</a></td><td align="center">WIDER FACE train</td><td align="center">WIDER FACE Val</td><td align="center">96.0%/ 94.8%/ 88.8%</td></tr></tbody></table><h4 id="性能曲线"><a href="#性能曲线" class="headerlink" title="性能曲线"></a>性能曲线</h4><p align="center"><img src="/imgs/eccv2018_pyramidbox/wider_pr_cruve_int_easy_val.jpg" width="280"> <img src="/imgs/eccv2018_pyramidbox/wider_pr_cruve_int_medium_val.jpg" width="280"> <img src="/imgs/eccv2018_pyramidbox/wider_pr_cruve_int_hard_val.jpg" width="280"><br>WIDER FACE Easy/Medium/Hard set</p></div><footer class="article-footer clearfix"><div class="article-catetags"><div class="article-tags"><span></span> <a href="/tags/Publications/">Publications</a></div></div><div class="article-share" id="share"><div data-url="http://tangxuvis.com/2018/09/04/eccv2018_pyramidbox/" data-title="News:1 paper accepted by ECCV2018 | Xu Tang&#39;s Homepages" data-tsina="" class="share clearfix"></div></div></footer></article><nav class="article-nav clearfix"><div class="prev"><a href="/2019/03/04/widerface201903/" title="News:1st place at hard set in Widerface leaderboard"><strong>上一篇：</strong><br><span>News:1st place at hard set in Widerface leaderboard</span></a></div><div class="next"><a href="/2018/07/04/cvpr2018_face_aging/" title="News:1 paper accepted by CVPR2018"><strong>下一篇：</strong><br><span>News:1 paper accepted by CVPR2018</span></a></div></nav></div><div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div><div id="toc" class="toc-aside"><strong class="toc-title">Contents</strong><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Pyramidbox-人脸检测"><span class="toc-number">1.</span> <span class="toc-text">Pyramidbox 人脸检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Table-of-Contents"><span class="toc-number">2.</span> <span class="toc-text">Table of Contents</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#简介"><span class="toc-number">2.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据准备"><span class="toc-number">2.2.</span> <span class="toc-text">数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型训练"><span class="toc-number">2.3.</span> <span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#下载预训练模型"><span class="toc-number">2.3.1.</span> <span class="toc-text">下载预训练模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#开始训练"><span class="toc-number">2.3.2.</span> <span class="toc-text">开始训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型评估"><span class="toc-number">2.4.</span> <span class="toc-text">模型评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型预测以及可视化"><span class="toc-number">2.5.</span> <span class="toc-text">模型预测以及可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型发布"><span class="toc-number">2.6.</span> <span class="toc-text">模型发布</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#性能曲线"><span class="toc-number">2.6.1.</span> <span class="toc-text">性能曲线</span></a></li></ol></li></ol></li></ol></div><div id="asidepart"><div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div><aside class="clearfix"><div class="github-card"><p class="asidetitle">Github Card</p><div class="github-card" data-github="takecareofbigboss" data-theme="medium"></div><script type="text/javascript" src="//cdn.jsdelivr.net/github-cards/latest/widget.js"></script></div><div class="tagslist"><p class="asidetitle">Tags</p><ul class="clearfix"><li><a href="/tags/Publications/" title="Publications">Publications<sup>5</sup></a></li><li><a href="/tags/Challenge/" title="Challenge">Challenge<sup>2</sup></a></li><li><a href="/tags/Leaderboard/" title="Leaderboard">Leaderboard<sup>2</sup></a></li></ul></div><div class="linkslist"><p class="asidetitle">Links</p><ul><li><a href="https://scholar.google.com/citations?user=grP24aAAAAAJ&hl=zh-CN" target="_blank" title="Google Scholars">Google Scholars</a></li></ul></div><div class="rsspart"><a href="/atom.xml" target="_blank" title="rss">RSS</a></div><div class="weiboshow"><p class="asidetitle">Weibo</p><iframe width="100%" height="119" class="share_self" frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=&verifier=&dpc=1"></iframe></div></aside></div></div><footer><div id="footer"><div class="line"><span></span><div class="author"></div></div><section class="info"><p>Hello, I&#39;m Xu Tang in Baidu Research.<br>Major in face detection/ object tracking/ object detection/ ...</p></section><div class="social-font" class="clearfix"><a href="http://weibo.com/1842792965" target="_blank" class="icon-weibo" title="微博"></a> <a href="https://github.com/takecareofbigboss" target="_blank" class="icon-github" title="github"></a> <a href="https://www.linkedin.com/in/旭-汤-b12626ba" target="_blank" class="icon-linkedin" title="linkedin"></a> <a href="http://www.zhihu.com/people/tang-xu-60-83" target="_blank" class="icon-zhihu" title="知乎"></a> <a href="mailto:tangxu02@baidu.com" target="_blank" class="icon-email" title="Email Me"></a></div><p class="copyright">Powered by Xu Tang and Theme by Xu Tang © 2019 <a href="/about" target="_blank" title="Xu Tang&#39;s Homepages">Xu Tang&#39;s Homepages</a></p></div></footer><script src="/js/jquery-2.0.3.min.js"></script><script src="/js/jquery.imagesloaded.min.js"></script><script src="/js/gallery.js"></script><script src="/js/jquery.qrcode-0.12.0.min.js"></script><script type="text/javascript">$(document).ready(function(){$(".navbar").click(function(){$("header nav").toggleClass("shownav")});var e=0;function n(){"number"==typeof window.innerWidth?e=window.innerWidth:document.documentElement&&document.documentElement.clientWidth&&(e=document.documentElement.clientWidth)}var s=$("#main"),a=$("#asidepart"),o=$(".closeaside"),d=$(".openaside");o.click(function(){a.addClass("fadeOut").css("display","none"),d.css("display","block").addClass("fadeIn"),s.addClass("moveMain")}),d.click(function(){d.css("display","none").removeClass("beforeFadeIn"),a.css("display","block").removeClass("fadeOut").addClass("fadeIn"),s.removeClass("moveMain")}),$(window).scroll(function(){d.css("top",Math.max(80,260-$(this).scrollTop()))}),n(),1024<=e&&o.click(),$(window).resize(function(){n(),1024<=e?$("header nav").removeClass("shownav"):(s.removeClass("moveMain"),a.css("display","block").removeClass("fadeOut"),d.css("display","none"),$("#toc.toc-aside").css("display","none"))})})</script><script type="text/javascript">$(document).ready(function(){var c=$(".article-content>iframe"),n=$(".article-content>embed"),o=($("#toc"),$("#toc.toc-aside")),e=$(".openaside"),i=$(".closeaside");0<c.length&&c.wrap('<div class="video-container" />'),0<n.length&&n.wrap('<div class="video-container" />'),i.click(function(){o.css("display","block").addClass("fadeIn")}),e.click(function(){o.css("display","none")}),$(window).scroll(function(){o.css("top",Math.max(140,320-$(this).scrollTop()))})})</script><script type="text/javascript">$(document).ready(function(){var e=$(".share"),a=e.attr("data-url"),t=encodeURIComponent(a),r=e.attr("data-title"),i=e.attr("data-tsina"),c=(e.attr("description"),['<div class="hoverqrcode clearfix"></div>','<a class="overlay" id="qrcode"></a>','<a href="https://www.facebook.com/sharer.php?u='+t+'" class="article-share-facebook" target="_blank" title="Facebook"></a>','<a href="https://twitter.com/intent/tweet?url='+t+'" class="article-share-twitter" target="_blank" title="Twitter"></a>','<a href="#qrcode" class="article-share-qrcode" title="微信"></a>','<a href="http://widget.renren.com/dialog/share?resourceUrl='+t+"&srcUrl="+t+"&title="+r+'" class="article-share-renren" target="_blank" title="人人"></a>','<a href="http://service.weibo.com/share/share.php?title='+r+"&url="+t+"&ralateUid="+i+'&searchPic=true&style=number" class="article-share-weibo" target="_blank" title="微博"></a>','<span title="Share to"></span>'].join(""));e.append(c),$(".hoverqrcode").hide();var o=0;$(window).resize(function(){$(".hoverqrcode").hide()}),$(".article-share-qrcode").click(function(){!function(){"number"==typeof window.innerWidth?o=window.innerWidth:document.documentElement&&document.documentElement.clientWidth&&(o=document.documentElement.clientWidth);var e=1024<o?200:100,t={render:"image",size:e,fill:"#2ca6cb",text:a,radius:.5,quiet:1},r=$(".article-share-qrcode").position();$(".hoverqrcode").empty().css("width",e).css("height",e).css("left",r.left-e/2+20).css("top",r.top-e-10).qrcode(t)}(),$(".hoverqrcode").toggle()}),$(".article-share-qrcode").hover(function(){},function(){$(".hoverqrcode").hide()})})</script><link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css"><script src="/fancybox/jquery.fancybox.pack.js"></script><script type="text/javascript">$(document).ready(function(){$(".article-content").each(function(a){$(this).find("img").each(function(){if(!$(this).parent().hasClass("fancybox")){var a=this.alt;a&&$(this).after('<span class="caption">'+a+"</span>"),$(this).wrap('<a href="'+this.src+'" title="'+a+'" class="fancybox"></a>')}}),$(this).find(".fancybox").each(function(){$(this).attr("rel","article"+a)})}),$.fancybox&&$(".fancybox").fancybox()})</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="//hm.baidu.com/hm.js?e6d1f421bbc9962127a50488f9ed37d1";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div id="totop"><a title="Back to Top"><img src="/img/scrollup.png"></a></div><script src="/js/totop.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body>