<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xu Tang&#39;s Homepages</title>
  
  <subtitle>Computer Vision &amp; Deep Learning &amp; Face Detection &amp; Object detection &amp; Object Tracking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tangxuvis.github.io/"/>
  <updated>2019-10-27T05:05:07.220Z</updated>
  <id>http://tangxuvis.github.io/</id>
  
  <author>
    <name>Xu Tang&#39;s Homepages</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>News:Our paper and code will be released in this page.</title>
    <link href="http://tangxuvis.github.io/2019/10/27/wider_challenge_2019/"/>
    <id>http://tangxuvis.github.io/2019/10/27/wider_challenge_2019/</id>
    <published>2019-10-27T04:36:59.000Z</published>
    <updated>2019-10-27T05:05:07.220Z</updated>
    
    <content type="html"><![CDATA[<p>1st place and 1 invited talk in <a href="https://wider-challenge.org/2019.html" target="_blank" rel="noopener">face detection track on ICCV Wider Challenge 2019</a>.<br>More details will be introduced in this page.</p><p>TBD …</p><p align="center"><img src="/imgs/iccv2019_wider_challenge/leaderboard.jpg" hspace="10"> <br>ICCV Wider Challenge优胜方案</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1st place and 1 invited talk in &lt;a href=&quot;https://wider-challenge.org/2019.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;face detection track on ICCV Wider Challenge 2019&lt;/a&gt;.&lt;br&gt;More details will be introduced in this page.&lt;/p&gt;
&lt;p&gt;TBD …&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/iccv2019_wider_challenge/leaderboard.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
ICCV Wider Challenge优胜方案
&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Challenge" scheme="http://tangxuvis.github.io/tags/Challenge/"/>
    
  </entry>
  
  <entry>
    <title>News:1 paper accepted by TIFS2019</title>
    <link href="http://tangxuvis.github.io/2019/09/04/tifs2019_daf/"/>
    <id>http://tangxuvis.github.io/2019/09/04/tifs2019_daf/</id>
    <published>2019-09-04T15:36:59.000Z</published>
    <updated>2019-10-05T18:28:46.148Z</updated>
    
    <content type="html"><![CDATA[<p>1 paper <a href="https://ieeexplore.ieee.org/document/8839888" target="_blank" rel="noopener">Progressively Refined Face Detection Through Semantics-Enriched Representation Learning</a> accepted by <a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-information-forensics-and-security" target="_blank" rel="noopener">IEEE Transactions on Information Forensics and Security (TIFS)</a> — CCF A.</p><p align="center"><img src="/imgs/tifs2019_daf/architecture.jpg" hspace="10"> <br></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 paper &lt;a href=&quot;https://ieeexplore.ieee.org/document/8839888&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Progressively Refined Face Detection Through Semantics-Enriched Representation Learning&lt;/a&gt; accepted by &lt;a href=&quot;https://signalprocessingsociety.org/publications-resources/ieee-transactions-information-forensics-and-security&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;IEEE Transactions on Information Forensics and Security (TIFS)&lt;/a&gt; — CCF A.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/tifs2019_daf/architecture.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Publications" scheme="http://tangxuvis.github.io/tags/Publications/"/>
    
  </entry>
  
  <entry>
    <title>News:1st place and 1 invited talk in face detection track on ICCV Wider Challenge 2019</title>
    <link href="http://tangxuvis.github.io/2019/08/04/iccv2019_wider_challenge/"/>
    <id>http://tangxuvis.github.io/2019/08/04/iccv2019_wider_challenge/</id>
    <published>2019-08-04T15:36:59.000Z</published>
    <updated>2019-10-05T17:18:37.416Z</updated>
    
    <content type="html"><![CDATA[<p>1st place and 1 invited talk in <a href="https://wider-challenge.org/2019.html" target="_blank" rel="noopener">face detection track on ICCV Wider Challenge 2019</a>. See you in Korea.</p><p align="center"><img src="/imgs/iccv2019_wider_challenge/leaderboard.jpg" hspace="10"> <br>ICCV Wider Challenge优胜方案</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1st place and 1 invited talk in &lt;a href=&quot;https://wider-challenge.org/2019.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;face detection track on ICCV Wider Challenge 2019&lt;/a&gt;. See you in Korea.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/iccv2019_wider_challenge/leaderboard.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
ICCV Wider Challenge优胜方案
&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Challenge" scheme="http://tangxuvis.github.io/tags/Challenge/"/>
    
  </entry>
  
  <entry>
    <title>News:1st place and 1 invited talk in VOT-ST2019</title>
    <link href="http://tangxuvis.github.io/2019/07/04/iccv2019_vot/"/>
    <id>http://tangxuvis.github.io/2019/07/04/iccv2019_vot/</id>
    <published>2019-07-04T15:36:59.000Z</published>
    <updated>2019-10-05T19:20:29.016Z</updated>
    
    <content type="html"><![CDATA[<p>1st place and 1 invited talk in <a href="http://www.votchallenge.net/vot2019/index.html" target="_blank" rel="noopener">VOT-ST2019</a>. See you in Korea.</p><p align="center"><img src="/imgs/iccv2019_vot/logo.jpg" hspace="10"> <br>vot优胜方案</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1st place and 1 invited talk in &lt;a href=&quot;http://www.votchallenge.net/vot2019/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;VOT-ST2019&lt;/a&gt;. See you in Korea.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/iccv2019_vot/logo.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
vot优胜方案
&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Challenge" scheme="http://tangxuvis.github.io/tags/Challenge/"/>
    
  </entry>
  
  <entry>
    <title>News:1 paper submited to arxiv</title>
    <link href="http://tangxuvis.github.io/2019/04/04/arxiv2019_dubox/"/>
    <id>http://tangxuvis.github.io/2019/04/04/arxiv2019_dubox/</id>
    <published>2019-04-04T15:36:59.000Z</published>
    <updated>2019-10-05T18:25:12.423Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1904.06883" target="_blank" rel="noopener">DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Traditional neural objection detection methods use multi-scale features that allow multiple detectors to perform<br>detecting tasks independently and in parallel. At the same time, with the handling of the prior box, the algorithm’s<br>ability to deal with scale invariance is enhanced. However, too many prior boxes and independent detectors will<br>increase the computational redundancy of the detection algorithm. In this study, we introduce Dubox, a new one-stage<br>approach that detects the objects without prior box. Working with multi-scale features, the designed dual scale residual unit makes dual scale detectors no longer run independently. The second scale detector learns the residual of the first. Dubox has enhanced the capacity of heuristic-guided that can further enable the first scale detector to maximize the detection of small targets and the second to detect objects that cannot be identified by the first one. Besides, for each scale detector, with the new classification-regression<br>progressive strapped loss makes our process not based on prior boxes. Integrating these strategies, our detection algorithm has achieved excellent performance in terms of speed and accuracy. Extensive experiments on the VOC, COCO object detection benchmark have confirmed the effectiveness of this algorithm.</p><p align="center"><img src="/imgs/arxiv2019_dubox/architecture.jpg" height="400" width="600" hspace="10"> <br></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.06883&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;Traditional neural objection detection methods use multi-scale features that allow multiple detectors to perform&lt;br&gt;detecting tasks independently and in parallel. At the same time, with the handling of the prior box, the algorithm’s&lt;br&gt;ability to deal with scale invariance is enhanced. However, too many prior boxes and independent detectors will&lt;br&gt;increase the computational redundancy of the detection algorithm. In this study, we introduce Dubox, a new one-stage&lt;br&gt;approach that detects the objects without prior box. Working with multi-scale features, the designed dual scale residual unit makes dual scale detectors no longer run independently. The second scale detector learns the residual of the first. Dubox has enhanced the capacity of heuristic-guided that can further enable the first scale detector to maximize the detection of small targets and the second to detect objects that cannot be identified by the first one. Besides, for each scale detector, with the new classification-regression&lt;br&gt;progressive strapped loss makes our process not based on prior boxes. Integrating these strategies, our detection algorithm has achieved excellent performance in terms of speed and accuracy. Extensive experiments on the VOC, COCO object detection benchmark have confirmed the effectiveness of this algorithm.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/arxiv2019_dubox/architecture.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Publications" scheme="http://tangxuvis.github.io/tags/Publications/"/>
    
  </entry>
  
  <entry>
    <title>News:1 paper submited to arxiv</title>
    <link href="http://tangxuvis.github.io/2019/03/04/arxiv2019_pyramidbox_plus_plus/"/>
    <id>http://tangxuvis.github.io/2019/03/04/arxiv2019_pyramidbox_plus_plus/</id>
    <published>2019-03-04T15:36:59.000Z</published>
    <updated>2019-10-05T18:25:16.838Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1904.00386.pdf" target="_blank" rel="noopener">PyramidBox++: High Performance Detector for Finding Tiny Face</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>With the rapid development of deep convolutional neural network, face detection has made great progress in recent years. WIDER FACE dataset, as a main benchmark, contributes greatly to this area. A large amount of methods have been put forward where PyramidBox designs an effective data augmentation strategy(Data-anchor-sampling) and context-based module for face detector. In this report, we improve each part to further boost the performance, including Balanced-dataanchor-sampling, Dual-PyramidAnchors and Dense Context Module. Specifically, Balanced-data-anchor-sampling obtains more uniform sampling of faces with different sizes. Dual-PyramidAnchors facilitate feature learning by introducing progressive anchor loss. Dense Context Module with dense connection not only enlarges receptive filed, but also passes information efficiently. Integrating these techniques, PyramidBox++ is constructed and achieves state-of-the-art performance in hard set.</p><p align="center"><img src="/imgs/arxiv2019_pyramidbox_plus_plus/architecture.jpg" height="400" width="600" hspace="10"> <br></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.00386.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PyramidBox++: High Performance Detector for Finding Tiny Face&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;With the rapid development of deep convolutional neural network, face detection has made great progress in recent years. WIDER FACE dataset, as a main benchmark, contributes greatly to this area. A large amount of methods have been put forward where PyramidBox designs an effective data augmentation strategy(Data-anchor-sampling) and context-based module for face detector. In this report, we improve each part to further boost the performance, including Balanced-dataanchor-sampling, Dual-PyramidAnchors and Dense Context Module. Specifically, Balanced-data-anchor-sampling obtains more uniform sampling of faces with different sizes. Dual-PyramidAnchors facilitate feature learning by introducing progressive anchor loss. Dense Context Module with dense connection not only enlarges receptive filed, but also passes information efficiently. Integrating these techniques, PyramidBox++ is constructed and achieves state-of-the-art performance in hard set.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/arxiv2019_pyramidbox_plus_plus/architecture.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Publications" scheme="http://tangxuvis.github.io/tags/Publications/"/>
    
  </entry>
  
  <entry>
    <title>News:1st place at hard set in Widerface leaderboard</title>
    <link href="http://tangxuvis.github.io/2019/03/04/widerface201903/"/>
    <id>http://tangxuvis.github.io/2019/03/04/widerface201903/</id>
    <published>2019-03-04T15:36:59.000Z</published>
    <updated>2019-10-05T18:28:24.208Z</updated>
    
    <content type="html"><![CDATA[<p>1st place at hard set in <a href="http://shuoyang1213.me/WIDERFACE/" target="_blank" rel="noopener">Widerface leaderboard</a>.</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1st place at hard set in &lt;a href=&quot;http://shuoyang1213.me/WIDERFACE/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Widerface leaderboard&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Leaderboard" scheme="http://tangxuvis.github.io/tags/Leaderboard/"/>
    
  </entry>
  
  <entry>
    <title>News:1 paper accepted by ECCV2018</title>
    <link href="http://tangxuvis.github.io/2018/09/04/eccv2018_pyramidbox/"/>
    <id>http://tangxuvis.github.io/2018/09/04/eccv2018_pyramidbox/</id>
    <published>2018-09-04T15:36:59.000Z</published>
    <updated>2019-10-05T18:25:25.220Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Pyramidbox-人脸检测"><a href="#Pyramidbox-人脸检测" class="headerlink" title="Pyramidbox 人脸检测"></a>Pyramidbox 人脸检测</h2><h2 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h2><ul><li><a href="#简介">简介</a></li><li><a href="#数据准备">数据准备</a></li><li><a href="#模型训练">模型训练</a></li><li><a href="#模型评估">模型评估</a></li><li><a href="#模型发布">模型发布</a></li></ul><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>人脸检测是经典的计算机视觉任务，非受控场景中的小脸、模糊和遮挡的人脸检测是这个方向上最有挑战的问题。<a href="https://arxiv.org/pdf/1803.07737.pdf" target="_blank" rel="noopener">PyramidBox</a> 是一种基于SSD的单阶段人脸检测器，它利用上下文信息解决困难人脸的检测问题。如下图所示，PyramidBox在六个尺度的特征图上进行不同层级的预测。该工作主要包括以下模块：LFPN、Pyramid Anchors、CPM、Data-anchor-sampling。具体可以参考该方法对应的论文 <a href="https://arxiv.org/pdf/1803.07737.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1803.07737.pdf</a> ，下面进行简要的介绍。</p><p align="center"><img src="/imgs/eccv2018_pyramidbox/architecture_of_pyramidbox.jpg" height="400" width="600" hspace="10"> <br>Pyramidbox 人脸检测模型</p><a id="more"></a><p><strong>LFPN</strong>: LFPN全称Low-level Feature Pyramid Networks, 在检测任务中，LFPN可以充分结合高层次的包含更多上下文的特征和低层次的包含更多纹理的特征。高层级特征被用于检测尺寸较大的人脸，而低层级特征被用于检测尺寸较小的人脸。为了将高层级特征整合到高分辨率的低层级特征上，我们从中间层开始做自上而下的融合，构建Low-level FPN。</p><p><strong>Pyramid Anchors</strong>: 该算法使用半监督解决方案来生成与人脸检测相关的具有语义的近似标签，提出基于anchor的语境辅助方法，它引入有监督的信息来学习较小的、模糊的和部分遮挡的人脸的语境特征。使用者可以根据标注的人脸标签，按照一定的比例扩充，得到头部的标签（上下左右各扩充1/2）和人体的标签（可自定义扩充比例）。</p><p><strong>CPM</strong>: CPM全称Context-sensitive Predict Module, 本方法设计了一种上下文敏感结构(CPM)来提高预测网络的表达能力。</p><p><strong>Data-anchor-sampling</strong>: 设计了一种新的采样方法，称作Data-anchor-sampling，该方法可以增加训练样本在不同尺度上的多样性。该方法改变训练样本的分布，重点关注较小的人脸。</p><p>Pyramidbox模型可以在以下示例图片上展示鲁棒的检测性能，该图有一千张人脸，该模型检测出其中的880张人脸。</p><p align="center"><img src="/imgs/eccv2018_pyramidbox/demo_img.jpg" height="300" width="500" hspace="10"> <br>Pyramidbox 人脸检测性能展示</p><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p>本教程使用 <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/" target="_blank" rel="noopener">WIDER FACE 数据集</a> 来进行模型的训练测试工作，官网给出了详尽的数据介绍。</p><p>WIDER FACE数据集包含32,203张图片，其中包含393,703个人脸，数据集的人脸在尺度、姿态、遮挡方面有较大的差异性。另外WIDER FACE数据集是基于61个场景归类的，然后针对每个场景，随机的挑选40%作为训练集，10%作为验证集，50%作为测试集。</p><p>首先，从官网训练集和验证集，放在<code>data</code>目录，官网提供了谷歌云和百度云下载地址，请依据情况自行下载。并下载训练集和验证集的标注信息:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./data/download.sh</span><br></pre></td></tr></table></figure><p>准备好数据之后，<code>data</code>目录如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">|-- download.sh</span><br><span class="line">|-- wider_face_split</span><br><span class="line">|   |-- readme.txt</span><br><span class="line">|   |-- wider_face_train_bbx_gt.txt</span><br><span class="line">|   |-- wider_face_val_bbx_gt.txt</span><br><span class="line">|   `-- ...</span><br><span class="line">|-- WIDER_train</span><br><span class="line">|   `-- images</span><br><span class="line">|       |-- 0--Parade</span><br><span class="line">|       ...</span><br><span class="line">|       `-- 9--Press_Conference</span><br><span class="line">`-- WIDER_val</span><br><span class="line">    `-- images</span><br><span class="line">        |-- 0--Parade</span><br><span class="line">        ...</span><br><span class="line">        `-- 9--Press_Conference</span><br></pre></td></tr></table></figure><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><h4 id="下载预训练模型"><a href="#下载预训练模型" class="headerlink" title="下载预训练模型"></a>下载预训练模型</h4><p>我们提供了预训练模型，模型是基于VGGNet的主干网络，使用如下命令下载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://paddlemodels.bj.bcebos.com/vgg_ilsvrc_16_fc_reduced.tar.gz</span><br><span class="line">tar -xf vgg_ilsvrc_16_fc_reduced.tar.gz &amp;&amp; rm -f vgg_ilsvrc_16_fc_reduced.tar.gz</span><br></pre></td></tr></table></figure><p>声明：该预训练模型转换自<a href="http://cs.unc.edu/~wliu/projects/ParseNet/VGG_ILSVRC_16_layers_fc_reduced.caffemodel" target="_blank" rel="noopener">Caffe</a>。不久，我们会发布自己预训练的模型。</p><h4 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h4><p><code>train.py</code> 是训练模块的主要执行程序，调用示例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -u train.py --batch_size=16 --pretrained_model=vgg_ilsvrc_16_fc_reduced</span><br></pre></td></tr></table></figure><ul><li><p>可以通过设置 <code>export CUDA_VISIBLE_DEVICES=0,1,2,3</code> 指定想要使用的GPU数量，<code>batch_size</code>默认设置为12或16。</p></li><li><p>更多的可选参数见:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py --<span class="built_in">help</span></span><br></pre></td></tr></table></figure></li><li><p>模型训练150轮以上可以收敛。用Nvidia Tesla P40 GPU 4卡并行，<code>batch_size=16</code>的配置，每轮训练大约40分钟，总共训练时长大约100小时</p></li></ul><p>模型训练所采用的数据增强：</p><p><strong>数据增强</strong>：数据的读取行为定义在 <code>reader.py</code> 中，所有的图片都会被缩放到640x640。在训练时还会对图片进行数据增强，包括随机扰动、翻转、裁剪等，和<a href="https://github.com/PaddlePaddle/models/blob/develop/fluid/PaddleCV/object_detection/README.md" target="_blank" rel="noopener">物体检测SSD算法</a>中数据增强类似，除此之外，增加了上面提到的Data-anchor-sampling:</p><p>  <strong>尺度变换(Data-anchor-sampling)</strong>：随机将图片尺度变换到一定范围的尺度，大大增强人脸的尺度变化。具体操作为根据随机选择的人脸高(height)和宽(width)，得到$v=\sqrt{width * height}$，判断$v$的值位于缩放区间$[16，32，64，128，256，512]$中的的哪一个。假设$v=45$，则选定$32&lt;v&lt;64$，以均匀分布的概率选取$[16，32，64]$中的任意一个值。若选中$64$，则该人脸的缩放区间在 $[64 / 2，min(v * 2, 64 * 2)]$中选定。</p><p><strong>注意</strong>：</p><ul><li>本次开源模型中CPM模块与论文中有些许不同，相比论文中CPM模块训练和测试速度更快。</li><li>Pyramid Anchors模块的body部分可以针对不同情况，进行相应的长宽设置来调参。同时face、head、body部分的loss对应的系数也可以通过调参优化。</li></ul><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><p>验证集的评估需要两个步骤：先预测出验证集的检测框和置信度，再利用WIDER FACE官方提供的评估脚本得到评估结果。</p><ul><li><p>预测检测结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -u widerface_eval.py --model_dir=output/159 --pred_dir=pred</span><br></pre></td></tr></table></figure><p>更多的可选参数:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -u widerface_eval.py --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><p><strong>注意</strong>： <code>widerface_eval.py</code>中<code>multi_scale_test_pyramid</code>可用可不用，由于Data-anchor-sampling的作用，更加密集的anchors对性能有更大的提升。</p></li><li><p>评估AP指标</p><p>下载官方评估脚本，评估average precision(AP)指标：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/support/eval_script/eval_tools.zip</span><br><span class="line">unzip eval_tools.zip &amp;&amp; rm -f eval_tools.zip</span><br></pre></td></tr></table></figure><p>修改<code>eval_tools/wider_eval.m</code>中检测结果保存的路径和将要画出的曲线名称：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 此处修改存放结果的文件夹名字</span><br><span class="line">pred_dir = &apos;./pred&apos;;  </span><br><span class="line"># 此处修改将要画出的曲线名称</span><br><span class="line">legend_name = &apos;Fluid-PyramidBox&apos;;</span><br></pre></td></tr></table></figure><p><code>wider_eval.m</code>是评估模块的主要执行程序，命令行式的运行命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matlab -nodesktop -nosplash -nojvm -r <span class="string">"run wider_eval.m;quit;"</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="模型预测以及可视化"><a href="#模型预测以及可视化" class="headerlink" title="模型预测以及可视化"></a>模型预测以及可视化</h3><p><code>widerface_eval.py</code>也可以用来做预测及可视化，调用示例如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python widerface_eval.py --infer=True --confs_threshold=0.15</span><br><span class="line"> --model_dir=output/159/ --image_path=data/WIDER_train/images/0--Parade/0_Parade_marchingband_1_219.jpg</span><br></pre></td></tr></table></figure><p>下图可视化了模型的预测结果：</p><p align="center"><img src="/imgs/eccv2018_pyramidbox/0_Parade_marchingband_1_356.jpg" height="300" width="300" hspace="10"><img src="/imgs/eccv2018_pyramidbox/28_Sports_Fan_Sports_Fan_28_770.jpg" height="300" width="300" hspace="10"><img src="/imgs/eccv2018_pyramidbox/4_Dancing_Dancing_4_194.jpg" height="300" width="300" hspace="10"><img src="/imgs/eccv2018_pyramidbox/12_Group_Group_12_Group_Group_12_935.jpg" height="300" width="300" hspace="10">  <br>Pyramidbox 预测可视化</p><h3 id="模型发布"><a href="#模型发布" class="headerlink" title="模型发布"></a>模型发布</h3><table><thead><tr><th align="center">模型</th><th align="center">预训练模型</th><th align="center">训练数据</th><th align="center">测试数据</th><th align="center">mAP</th></tr></thead><tbody><tr><td align="center"><a href="http://paddlemodels.bj.bcebos.com/PyramidBox_WiderFace.tar.gz" target="_blank" rel="noopener">Pyramidbox-v1-SSD 640x640</a></td><td align="center"><a href="http://paddlemodels.bj.bcebos.com/vgg_ilsvrc_16_fc_reduced.tar.gz" target="_blank" rel="noopener">VGGNet</a></td><td align="center">WIDER FACE train</td><td align="center">WIDER FACE Val</td><td align="center">96.0%/ 94.8%/ 88.8%</td></tr></tbody></table><h4 id="性能曲线"><a href="#性能曲线" class="headerlink" title="性能曲线"></a>性能曲线</h4><p align="center">    <img src="/imgs/eccv2018_pyramidbox/wider_pr_cruve_int_easy_val.jpg" width="280">    <img src="/imgs/eccv2018_pyramidbox/wider_pr_cruve_int_medium_val.jpg" width="280">    <img src="/imgs/eccv2018_pyramidbox/wider_pr_cruve_int_hard_val.jpg" width="280"><br>WIDER FACE Easy/Medium/Hard set</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Pyramidbox-人脸检测&quot;&gt;&lt;a href=&quot;#Pyramidbox-人脸检测&quot; class=&quot;headerlink&quot; title=&quot;Pyramidbox 人脸检测&quot;&gt;&lt;/a&gt;Pyramidbox 人脸检测&lt;/h2&gt;&lt;h2 id=&quot;Table-of-Contents&quot;&gt;&lt;a href=&quot;#Table-of-Contents&quot; class=&quot;headerlink&quot; title=&quot;Table of Contents&quot;&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#简介&quot;&gt;简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#数据准备&quot;&gt;数据准备&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#模型训练&quot;&gt;模型训练&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#模型评估&quot;&gt;模型评估&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#模型发布&quot;&gt;模型发布&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h3&gt;&lt;p&gt;人脸检测是经典的计算机视觉任务，非受控场景中的小脸、模糊和遮挡的人脸检测是这个方向上最有挑战的问题。&lt;a href=&quot;https://arxiv.org/pdf/1803.07737.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PyramidBox&lt;/a&gt; 是一种基于SSD的单阶段人脸检测器，它利用上下文信息解决困难人脸的检测问题。如下图所示，PyramidBox在六个尺度的特征图上进行不同层级的预测。该工作主要包括以下模块：LFPN、Pyramid Anchors、CPM、Data-anchor-sampling。具体可以参考该方法对应的论文 &lt;a href=&quot;https://arxiv.org/pdf/1803.07737.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1803.07737.pdf&lt;/a&gt; ，下面进行简要的介绍。&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/eccv2018_pyramidbox/architecture_of_pyramidbox.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
Pyramidbox 人脸检测模型
&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Publications" scheme="http://tangxuvis.github.io/tags/Publications/"/>
    
  </entry>
  
  <entry>
    <title>News:1 paper accepted by CVPR2018</title>
    <link href="http://tangxuvis.github.io/2018/07/04/cvpr2018_face_aging/"/>
    <id>http://tangxuvis.github.io/2018/07/04/cvpr2018_face_aging/</id>
    <published>2018-07-04T15:36:59.000Z</published>
    <updated>2019-10-05T18:25:20.939Z</updated>
    
    <content type="html"><![CDATA[<p>This repo is the official open source of <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Face_Aging_With_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Face Aging with Identity-Preserved Conditional Generative Adversarial Networks, CVPR 2018</a> by Zongwei Wang, Xu Tang, Weixin Luo and Shenghua Gao.<br>It is implemented in tensorflow. Please follow the instructions to run the code.<br><img src="/imgs/cvpr2018_face_aging/cvpr2018_framework.jpeg" alt="scalars_framework"></p><a id="more"></a><h2 id="1-Installation"><a href="#1-Installation" class="headerlink" title="1. Installation"></a>1. Installation</h2><ul><li>Install 3rd-package dependencies of python (listed in requirements.txt)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensorflow-gpu==1.4.1</span><br><span class="line">scipy==1.0.0</span><br><span class="line">opencv-python==3.3.0.10</span><br><span class="line">numpy==1.11.0</span><br><span class="line">Pillow==5.1.0</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><ul><li>Other libraries<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CUDA 8.0</span><br><span class="line">Cudnn 6.0</span><br></pre></td></tr></table></figure></li></ul><h2 id="2-Download-datasets"><a href="#2-Download-datasets" class="headerlink" title="2. Download datasets"></a>2. Download datasets</h2><p>We use the Cross-Age Celebrity Dataset for training and Evaluation. More details about this dataset, please refer to (<a href="http://bcsiriuschen.github.io/CARC/" target="_blank" rel="noopener">http://bcsiriuschen.github.io/CARC/</a>). After face detection, aligning and center cropping, we split<br>images into 5 age groups: 11-20, 21-30, 31-40, 41-50 and 50+.</p><h2 id="3-Test-on-saved-models"><a href="#3-Test-on-saved-models" class="headerlink" title="3. Test on saved models"></a>3. Test on saved models</h2><p>Download the trained face aging model(<a href="https://1drv.ms/u/s!AlUWwwOcwDWobCqmuFyKGIt4qaA" target="_blank" rel="noopener">https://1drv.ms/u/s!AlUWwwOcwDWobCqmuFyKGIt4qaA</a>) and place models files in checkpoints/0_conv5_lsgan_transfer_g75_0.5f-4_a30. </p><ul><li>Test images are in images/test, and some training images that belong to 11-20 age group are in images/train.<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">* Running the sript to get aged faces</span><br><span class="line">    python test.py</span><br></pre></td></tr></table></figure></li></ul><h2 id="4-Train-from-scratch"><a href="#4-Train-from-scratch" class="headerlink" title="4. Train from scratch"></a>4. Train from scratch</h2><ul><li>Firstly, download the pre-trained alexnet model(<a href="https://1drv.ms/u/s!AlUWwwOcwDWobkptownyu5fjlfU" target="_blank" rel="noopener">https://1drv.ms/u/s!AlUWwwOcwDWobkptownyu5fjlfU</a>) and age classfication model(<a href="https://1drv.ms/f/s!AlUWwwOcwDWocX-Z0IJft_VbcoQ" target="_blank" rel="noopener">https://1drv.ms/f/s!AlUWwwOcwDWocX-Z0IJft_VbcoQ</a>). Then unzip these files and place model files in checkpoints/pre_trained.<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">python age_lsgan_transfer.py \</span><br><span class="line">  --gan_loss_weight=75 \</span><br><span class="line">  --fea_loss_weight=0.5e-4 \</span><br><span class="line">  --age_loss_weight=30 \</span><br><span class="line">  --fea_layer_name=conv5 \</span><br><span class="line">  --checkpoint_dir=./checkpoints/age/0_conv5_lsgan_transfer_g75_0.5f-4_a30 \</span><br><span class="line">  --sample_dir=age/0_conv5_lsgan_transfer_g75_0.5f-4_a30</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh age_lsgan_transfer.py</span><br></pre></td></tr></table></figure><ul><li>You can change the hyperparameters to adapt to your own dataset.</li></ul><h2 id="5-Experiment-results"><a href="#5-Experiment-results" class="headerlink" title="5. Experiment results"></a>5. Experiment results</h2><ul><li>The aging effect of different methods. Within each dot box, the first row is our result.<img src="/imgs/cvpr2018_face_aging/method_comp.jpeg" alt="scalars_method_comparison"></li><li>The aging effect of different age classification loss weights.<img src="/imgs/cvpr2018_face_aging/age_effect.jpeg" alt="scalars_age_loss_weight"></li><li>The aging effect of different feature layer.<img src="/imgs/cvpr2018_face_aging/layer.jpeg" alt="scalars_layer"></li></ul><h2 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h2><p>If you find this useful, please cite our work as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@INPROCEEDINGS&#123;wang2018face_aging, </span><br><span class="line">    author=&#123;Z. Wang and X. Tang, W. Luo and S. Gao&#125;, </span><br><span class="line">    booktitle=&#123;2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)&#125;, </span><br><span class="line">    title=&#123;Face Aging with Identity-Preserved Conditional Generative Adversarial Networks&#125;, </span><br><span class="line">    year=&#123;2018&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This repo is the official open source of &lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Face_Aging_With_CVPR_2018_paper.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Face Aging with Identity-Preserved Conditional Generative Adversarial Networks, CVPR 2018&lt;/a&gt; by Zongwei Wang, Xu Tang, Weixin Luo and Shenghua Gao.&lt;br&gt;It is implemented in tensorflow. Please follow the instructions to run the code.&lt;br&gt;&lt;img src=&quot;/imgs/cvpr2018_face_aging/cvpr2018_framework.jpeg&quot; alt=&quot;scalars_framework&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Publications" scheme="http://tangxuvis.github.io/tags/Publications/"/>
    
  </entry>
  
  <entry>
    <title>News:1st place at all 6 set in WiderFace Leaderboard and 1st place at discrete score in FDDB Leaderboard</title>
    <link href="http://tangxuvis.github.io/2018/03/04/widerface201803/"/>
    <id>http://tangxuvis.github.io/2018/03/04/widerface201803/</id>
    <published>2018-03-04T15:36:59.000Z</published>
    <updated>2019-10-05T17:19:03.542Z</updated>
    
    <content type="html"><![CDATA[<p>1st place at all 6 set in <a href="http://shuoyang1213.me/WIDERFACE/" target="_blank" rel="noopener">WiderFace Leaderboard</a>.<br>1st place at discrete score in <a href="http://vis-www.cs.umass.edu/fddb/" target="_blank" rel="noopener">FDDB Leaderboard</a>.</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1st place at all 6 set in &lt;a href=&quot;http://shuoyang1213.me/WIDERFACE/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;WiderFace Leaderboard&lt;/a&gt;.&lt;br&gt;1st place at discrete score in &lt;a href=&quot;http://vis-www.cs.umass.edu/fddb/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;FDDB Leaderboard&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Leaderboard" scheme="http://tangxuvis.github.io/tags/Leaderboard/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://tangxuvis.github.io/2018/02/04/hello-world/"/>
    <id>http://tangxuvis.github.io/2018/02/04/hello-world/</id>
    <published>2018-02-04T15:36:59.000Z</published>
    <updated>2019-10-05T17:11:59.235Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
</feed>
