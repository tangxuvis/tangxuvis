<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xu Tang&#39;s Homepages</title>
  
  <subtitle>Computer Vision &amp; Deep Learning &amp; Face Detection &amp; Object detection &amp; Object Tracking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tangxuvis.github.io/"/>
  <updated>2021-07-06T01:42:43.979Z</updated>
  <id>http://tangxuvis.github.io/</id>
  
  <author>
    <name>Xu Tang&#39;s Homepages</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>News---paper accepted by ACMMM21: Decoupled IoU Regression for Object Detection</title>
    <link href="http://tangxuvis.github.io/2021/07/04/acmmm2021_v1/"/>
    <id>http://tangxuvis.github.io/2021/07/04/acmmm2021_v1/</id>
    <published>2021-07-04T02:45:08.000Z</published>
    <updated>2021-07-06T01:42:43.979Z</updated>
    
    <content type="html"><![CDATA[<p>##Decoupled IoU Regression for Object Detection</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Non-maximum suppression(NMS) is widely used in object detection pipelines for removing duplicated bounding boxes. The inconsistency between the confidence for NMS and the real localization confidence seriously affects detection performance. Prior works propose to predict Intersection-over-Union (IoU) between bounding boxes and corresponding ground-truths to improve NMS, while accurately predicting IoU is still a challenging problem. We argue that the complex definition of IoU and feature misalignment make it difficult to predict IoU accurately. In this paper, we propose a novel Decoupled IoU Regression(DIR) model to handle these problems. The proposed DIR decouples the traditional localization confidence metric IoU into two new metrics, Purity and Integrity. Purity reflects the proportion of the object area in the detected bounding box, and Integrity refers to the completeness of the detected object area. Separately predicting Purity and Integrity can divide the complex mapping between the bounding box and its IoU into two clearer mappings and model them independently. In addition, a simple but effective feature realignment approach is also introduced to make the IoU regressor work in a hindsight manner, which can make the target mapping more stable. The proposed DIR can be conveniently integrated with existing two-stage detectors and significantly improve their performance. Through a simple implementation of DIR with Faster R-CNN, we obtain 41.9% AP on MS COCO benchmark under ResNet101 backbone, which outperforms previous methods by a large margin and achieves state-of-the-art.</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;##Decoupled IoU Regression for Object Detection&lt;/p&gt;
&lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;Non-maximum suppression(NMS) is widely used in object detection pipelines for removing duplicated bounding boxes. The inconsistency between the confidence for NMS and the real localization confidence seriously affects detection performance. Prior works propose to predict Intersection-over-Union (IoU) between bounding boxes and corresponding ground-truths to improve NMS, while accurately predicting IoU is still a challenging problem. We argue that the complex definition of IoU and feature misalignment make it difficult to predict IoU accurately. In this paper, we propose a novel Decoupled IoU Regression(DIR) model to handle these problems. The proposed DIR decouples the traditional localization confidence metric IoU into two new metrics, Purity and Integrity. Purity reflects the proportion of the object area in the detected bounding box, and Integrity refers to the completeness of the detected object area. Separately predicting Purity and Integrity can divide the complex mapping between the bounding box and its IoU into two clearer mappings and model them independently. In addition, a simple but effective feature realignment approach is also introduced to make the IoU regressor work in a hindsight manner, which can make the target mapping more stable. The proposed DIR can be conveniently integrated with existing two-stage detectors and significantly improve their performance. Through a simple implementation of DIR with Faster R-CNN, we obtain 41.9% AP on MS COCO benchmark under ResNet101 backbone, which outperforms previous methods by a large margin and achieves state-of-the-art.&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>News---paper accepted by ACMMM21: Deep Interactive Video Inpainting: an Invisibility Cloak for Harry Potter</title>
    <link href="http://tangxuvis.github.io/2021/07/04/acmmm2021/"/>
    <id>http://tangxuvis.github.io/2021/07/04/acmmm2021/</id>
    <published>2021-07-04T01:45:08.000Z</published>
    <updated>2021-07-06T01:42:26.750Z</updated>
    
    <content type="html"><![CDATA[<p>##Deep Interactive Video Inpainting: an Invisibility Cloak for Harry Potter</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>In this paper, we propose a new task of deep interactive video inpainting and an application for users interact with the machine. To our knowledge, this is the first deep learning based interactive video inpainting work that only uses a free form user input as guidance (i.e. scribbles) instead of mask annotations for each frame, which has academic, entertainment, and commercial value. With users’ scribbles on a certain frame, it can simultaneously perform interactive video object segmentation and video inpainting tasks throughout the whole video. We utilize a shared spatial-temporal memory module, which combines the interactive video object segmentation and video inpainting tasks into an end-to-end pipeline. In our framework, the past frames with object masks(either the user’s scribbles or the predicted masks) form an external memory, and the current frame as the query is segmented and inpainted using the information in the shared memory. Furthermore, our method allows users to iteratively refine the segmentation results, which can effectively improve the inpainting results where the video object segmentation fails, thus allowing users to obtain high-quality video inpainting results even on challenging sequences. Qualitative and quantitative experimental results demonstrate the superiority of our approach.</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;##Deep Interactive Video Inpainting: an Invisibility Cloak for Harry Potter&lt;/p&gt;
&lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;p&gt;In this paper, we propose a new task of deep interactive video inpainting and an application for users interact with the machine. To our knowledge, this is the first deep learning based interactive video inpainting work that only uses a free form user input as guidance (i.e. scribbles) instead of mask annotations for each frame, which has academic, entertainment, and commercial value. With users’ scribbles on a certain frame, it can simultaneously perform interactive video object segmentation and video inpainting tasks throughout the whole video. We utilize a shared spatial-temporal memory module, which combines the interactive video object segmentation and video inpainting tasks into an end-to-end pipeline. In our framework, the past frames with object masks(either the user’s scribbles or the predicted masks) form an external memory, and the current frame as the query is segmented and inpainted using the information in the shared memory. Furthermore, our method allows users to iteratively refine the segmentation results, which can effectively improve the inpainting results where the video object segmentation fails, thus allowing users to obtain high-quality video inpainting results even on challenging sequences. Qualitative and quantitative experimental results demonstrate the superiority of our approach.&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>阿里文娱速看短视频自动化生产解决方案</title>
    <link href="http://tangxuvis.github.io/2021/05/27/ali_sukan/"/>
    <id>http://tangxuvis.github.io/2021/05/27/ali_sukan/</id>
    <published>2021-05-27T06:54:11.000Z</published>
    <updated>2021-07-05T11:42:24.743Z</updated>
    
    <content type="html"><![CDATA[<p align="center"><img src="/imgs/alibaba/alibaba_sukan.jpg" hspace="10"> <br></p><p><a href="https://mp.weixin.qq.com/s/rajmVlbWYl6jLjT_0Ztzaw" target="_blank" rel="noopener">阿里文娱速看短视频自动化生产解决方案</a></p><p>随着用户的时间碎片化程度加剧，视频“由长变短”成为一种趋势，信息流场景下的短视频消费需求日益增长，优酷每年为用户提供大量优质视频资源，具备天然的“由长变短”优势，并通过算法研究在速看短视频的自动化生产方面取得突破。</p><p>AI自动剪辑的目标是通过算法手段全自动或半自动进行视频剪辑，借助机器的批量化优势实现批量化生产，能够提升内容生产效率，提升短视频运营和分发效率。目前全网人工短视频生产集中在头部IP，AI自动剪辑可以为腰、尾部版权IP内容进行定向供货，带来新的流量增长点。</p><p>目前优酷已经将AI算法能力赋能到了多个业务场景，比如优酷弹幕看点提取、视频理解标签、剧集前情提要、智能封面图、视频速看解说等。例如，智能封面图能力不但支持短视频智能生产，还作为媒资的基础服务开放给UPGC，应用于优酷号上传、优酷搜索、短/小视频推荐等场景。</p><p>与此同时，还搭建了前情提要“机器生产+人工审核+广告生成”的生产链路，相比纯人工生产的前情提要，新链路将生产时长从天级别压缩到分钟级别，极大地提高了生产效率。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/alibaba/alibaba_sukan.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/rajmVlbWYl6jLjT_0Ztzaw&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;阿里文娱速看短视频自动化生产解决方案&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;随着用户的时间碎片化程度加剧，视频“由长变短”成为一种趋势，信息流场景下的短视频消费需求日益增长，优酷每年为用户提供大量优质视频资源，具备天然的“由长变短”优势，并通过算法研究在速看短视频的自动化生产方面取得突破。&lt;/p&gt;
&lt;p&gt;AI自动剪辑的目标是通过算法手段全自动或半自动进行视频剪辑，借助机器的批量化优势实现批量化生产，能够提升内容生产效率，提升短视频运营和分发效率。目前全网人工短视频生产集中在头部IP，AI自动剪辑可以为腰、尾部版权IP内容进行定向供货，带来新的流量增长点。&lt;/p&gt;
&lt;p&gt;目前优酷已经将AI算法能力赋能到了多个业务场景，比如优酷弹幕看点提取、视频理解标签、剧集前情提要、智能封面图、视频速看解说等。例如，智能封面图能力不但支持短视频智能生产，还作为媒资的基础服务开放给UPGC，应用于优酷号上传、优酷搜索、短/小视频推荐等场景。&lt;/p&gt;
&lt;p&gt;与此同时，还搭建了前情提要“机器生产+人工审核+广告生成”的生产链路，相比纯人工生产的前情提要，新链路将生产时长从天级别压缩到分钟级别，极大地提高了生产效率。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>浙大蔡登老师实验室分享---视频多模态理解&amp;互动特效的研究与技术实践</title>
    <link href="http://tangxuvis.github.io/2021/04/20/zheda_caideng_2021/"/>
    <id>http://tangxuvis.github.io/2021/04/20/zheda_caideng_2021/</id>
    <published>2021-04-20T06:54:11.000Z</published>
    <updated>2021-05-17T10:30:42.319Z</updated>
    
    <content type="html"><![CDATA[<p>浙大蔡登老师实验室分享—视频多模态理解&amp;互动特效的研究与技术实践，相关ppt参考链接：<a href="https://pan.baidu.com/s/1n5XJB92uHqPDXWj06LIhdA" target="_blank" rel="noopener">https://pan.baidu.com/s/1n5XJB92uHqPDXWj06LIhdA</a><br>提取码：0V9i （已报备）</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;浙大蔡登老师实验室分享—视频多模态理解&amp;amp;互动特效的研究与技术实践，相关ppt参考链接：&lt;a href=&quot;https://pan.baidu.com/s/1n5XJB92uHqPDXWj06LIhdA&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://pan.baidu.com/s/1n5XJB92uHqPDXWj06LIhdA&lt;/a&gt;&lt;br&gt;提取码：0V9i （已报备）&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>DataFun峰会知识图谱与智能创作论坛---阿里文娱视频智能生产技术实践</title>
    <link href="http://tangxuvis.github.io/2021/03/27/jieshuo_datafun_2021/"/>
    <id>http://tangxuvis.github.io/2021/03/27/jieshuo_datafun_2021/</id>
    <published>2021-03-27T06:54:11.000Z</published>
    <updated>2021-05-17T09:45:53.325Z</updated>
    
    <content type="html"><![CDATA[<p align="center"><img src="/imgs/datafun_2021/datafun.jpg" hspace="10"> <br></p><p>DataFun峰会知识图谱与智能创作论坛技术分享，<a href="https://mp.weixin.qq.com/s/BmoCCLtUcZXAi23JEpQeaQ" target="_blank" rel="noopener">阿里文娱视频智能生产技术实践</a>。</p><p>分享我们在视频智能生产和创作上，近期的进展，包括视频切条、视频混剪&amp;二创、视频浓缩、视频解说、文本视频化等。</p><p>相关ppt见链接：<a href="https://pan.baidu.com/s/1n5XJB92uHqPDXWj06LIhdA" target="_blank" rel="noopener">https://pan.baidu.com/s/1n5XJB92uHqPDXWj06LIhdA</a><br>提取码：0V9i （已报备）</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/datafun_2021/datafun.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;

&lt;p&gt;DataFun峰会知识图谱与智能创作论坛技术分享，&lt;a href=&quot;https://mp.weixin.qq.com/s/BmoCCLtUcZXAi23JEpQeaQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;阿里文娱视频智能生产技术实践&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;分享我们在视频智能生产和创作上，近期的进展，包括视频切条、视频混剪&amp;amp;二创、视频浓缩、视频解说、文本视频化等。&lt;/p&gt;
&lt;p&gt;相关ppt见链接：&lt;a href=&quot;https://pan.baidu.com/s/1n5XJB92uHqPDXWj06LIhdA&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://pan.baidu.com/s/1n5XJB92uHqPDXWj06LIhdA&lt;/a&gt;&lt;br&gt;提取码：0V9i （已报备）&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>News:Join in Alibaba</title>
    <link href="http://tangxuvis.github.io/2020/06/22/join-alibaba/"/>
    <id>http://tangxuvis.github.io/2020/06/22/join-alibaba/</id>
    <published>2020-06-22T06:54:11.000Z</published>
    <updated>2021-05-17T10:38:30.536Z</updated>
    
    <content type="html"><![CDATA[<p>很荣幸能够加入阿里巴巴，希望在接下来的几年能够继续努力、高效、快乐的工作，做出更多有意义、有价值、有影响力的科研成果和产品。<br>目前我在阿里文娱负责视频智能创作&amp;互动特效方向。现主要研究方向包含两大块—视频智能创作：Video Summary/ Video Grounding/ 视频智能解说/ 文本视频化/ Text Video Retrieval，人脸互动特效：人脸检测跟踪/人脸编辑/ 人脸风格化/人脸换脸/人脸属性等。</p><p align="center"><img src="/imgs/alibaba/alibaba.jpg" hspace="10"> <br></p><p>另外，本人在招Research Intern和社招。欢迎感兴趣的同学可以邮件（或微信）联系我：<a href="mailto:buhui.tx@alibaba-inc.com" target="_blank" rel="noopener">buhui.tx@alibaba-inc.com</a><br>期待你的加入。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很荣幸能够加入阿里巴巴，希望在接下来的几年能够继续努力、高效、快乐的工作，做出更多有意义、有价值、有影响力的科研成果和产品。&lt;br&gt;目前我在阿里文娱负责视频智能创作&amp;amp;互动特效方向。现主要研究方向包含两大块—视频智能创作：Video Summary/ Video Grounding/ 视频智能解说/ 文本视频化/ Text Video Retrieval，人脸互动特效：人脸检测跟踪/人脸编辑/ 人脸风格化/人脸换脸/人脸属性等。&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/alibaba/alibaba.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;

&lt;p&gt;另外，本人在招Research Intern和社招。欢迎感兴趣的同学可以邮件（或微信）联系我：&lt;a href=&quot;mailto:buhui.tx@alibaba-inc.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;buhui.tx@alibaba-inc.com&lt;/a&gt;&lt;br&gt;期待你的加入。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>字节跳动---人脸检测技术发展及小尺度人脸检测“框”实践</title>
    <link href="http://tangxuvis.github.io/2020/06/19/bytedance-talk/"/>
    <id>http://tangxuvis.github.io/2020/06/19/bytedance-talk/</id>
    <published>2020-06-19T06:54:11.000Z</published>
    <updated>2020-09-16T02:05:07.499Z</updated>
    
    <content type="html"><![CDATA[<p>荣幸受邀，在字节跳动分享了自己过往在人脸检测领域的成果和思考。</p><p align="center"><img src="/imgs/bytedance/bytedance.jpg" hspace="10"> <br></p><p>相关资料参考链接：<a href="https://www.techbeat.net/talk-info?id=384" target="_blank" rel="noopener">techbeat官网链接</a></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;荣幸受邀，在字节跳动分享了自己过往在人脸检测领域的成果和思考。&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/bytedance/bytedance.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;

&lt;p&gt;相关资料参考链接：&lt;a href=&quot;https://www.techbeat.net/talk-info?id=384&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;techbeat官网链接&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>News:1 oral papers accepted by ACMMM20</title>
    <link href="http://tangxuvis.github.io/2020/05/16/acmmm2020-oral/"/>
    <id>http://tangxuvis.github.io/2020/05/16/acmmm2020-oral/</id>
    <published>2020-05-16T01:45:08.000Z</published>
    <updated>2020-09-16T02:05:14.267Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/2008.11769" target="_blank" rel="noopener">Learning Global Structure Consistency for Robust Object Tracking</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Fast appearance variations and the distractions of similar objects are two of the most challenging problems in visual object tracking. Unlike many existing trackers that focus on modeling only the target, in this work, we consider the \emph{transient variations of the whole scene}. The key insight is that the object correspondence and spatial layout of the whole scene are consistent (i.e., global structure consistency) in consecutive frames which helps to disambiguate the target from distractors. Moreover, modeling transient variations enables to localize the target under fast variations. Specifically, we propose an effective and efficient short-term model that learns to exploit the global structure consistency in a short time and thus can handle fast variations and distractors. Since short-term modeling falls short of handling occlusion and out of the views, we adopt the long-short term paradigm and use a long-term model that corrects the short-term model when it drifts away from the target or the target is not present. These two components are carefully combined to achieve the balance of stability and plasticity during tracking. We empirically verify that the proposed tracker can tackle the two challenging scenarios and validate it on large scale benchmarks. Remarkably, our tracker improves state-of-the-art-performance on VOT2018 from 0.440 to 0.460, GOT-10k from 0.611 to 0.640, and NFS from 0.619 to 0.629.</p><p align="center"><img src="/imgs/acmmm2020/acmmm_oral.jpg" hspace="10"> <br></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2008.11769&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Learning Global Structure Consistency for Robust Object Tracking&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;Fast appearance variations and the distractions of similar objects are two of the most challenging problems in visual object tracking. Unlike many existing trackers that focus on modeling only the target, in this work, we consider the \emph{transient variations of the whole scene}. The key insight is that the object correspondence and spatial layout of the whole scene are consistent (i.e., global structure consistency) in consecutive frames which helps to disambiguate the target from distractors. Moreover, modeling transient variations enables to localize the target under fast variations. Specifically, we propose an effective and efficient short-term model that learns to exploit the global structure consistency in a short time and thus can handle fast variations and distractors. Since short-term modeling falls short of handling occlusion and out of the views, we adopt the long-short term paradigm and use a long-term model that corrects the short-term model when it drifts away from the target or the target is not present. These two components are carefully combined to achieve the balance of stability and plasticity during tracking. We empirically verify that the proposed tracker can tackle the two challenging scenarios and validate it on large scale benchmarks. Remarkably, our tracker improves state-of-the-art-performance on VOT2018 from 0.440 to 0.460, GOT-10k from 0.611 to 0.640, and NFS from 0.619 to 0.629.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/acmmm2020/acmmm_oral.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>News:2nd place in CVPR UG2+ Challenge 2020</title>
    <link href="http://tangxuvis.github.io/2020/05/16/cvpr2020-ug2challenge/"/>
    <id>http://tangxuvis.github.io/2020/05/16/cvpr2020-ug2challenge/</id>
    <published>2020-05-16T01:35:59.000Z</published>
    <updated>2020-09-16T03:07:13.415Z</updated>
    
    <content type="html"><![CDATA[<p>2nd place in the (SEMI-)SUPERVISED FACE DETECTION IN LOW LIGHT CONDITIONS track on <a href="http://cvpr2020.ug2challenge.org/leaderboard20_t1.html" target="_blank" rel="noopener">CVPR UG2+ Challenge 2020</a>.<br>The <a href="https://drive.google.com/file/d/1ogI0veozUtUO0qIgG-ce_jz8_hBeKV1Z/view" target="_blank" rel="noopener">fact sheet</a> can be seen here.</p><p align="center"><img src="/imgs/cvpr2020_ug2_challenge/hambox.jpg" hspace="10"> <br></p><p align="center"><img src="/imgs/cvpr2020_ug2_challenge/leaderboard.jpg" hspace="10"> <br></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2nd place in the (SEMI-)SUPERVISED FACE DETECTION IN LOW LIGHT CONDITIONS track on &lt;a href=&quot;http://cvpr2020.ug2challenge.org/leaderboard20_t1.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CVPR UG2+ Challenge 2020&lt;/a&gt;.&lt;br&gt;The &lt;a href=&quot;https://drive.google.com/file/d/1ogI0veozUtUO0qIgG-ce_jz8_hBeKV1Z/view&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;fact sheet&lt;/a&gt; can be seen here.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/cvpr2020_ug2_challenge/hambox.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/cvpr2020_ug2_challenge/leaderboard.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Challenge" scheme="http://tangxuvis.github.io/tags/Challenge/"/>
    
  </entry>
  
  <entry>
    <title>将门---人脸检测技术发展及小尺度人脸检测“框”实践</title>
    <link href="http://tangxuvis.github.io/2020/05/07/jiangmen-talk/"/>
    <id>http://tangxuvis.github.io/2020/05/07/jiangmen-talk/</id>
    <published>2020-05-07T06:54:11.000Z</published>
    <updated>2021-05-17T08:57:58.840Z</updated>
    
    <content type="html"><![CDATA[<p align="center"><img src="/imgs/jiangmen/jiangmen.jpg" hspace="10"> <br></p><p>将门社区技术分享，<a href="https://www.techbeat.net/talk-info?id=384" target="_blank" rel="noopener">techbeat官网链接</a> / <a href="https://www.bilibili.com/video/BV1qk4y1z7cV" target="_blank" rel="noopener">bilibili直播链接</a>。<br>ppt文件分享，<a href="https://pan.baidu.com/s/1cYtc_aDyDFogjTl47VyfvA" target="_blank" rel="noopener">Baidu Cloud</a><br>链接:<a href="https://pan.baidu.com/s/1cYtc_aDyDFogjTl47VyfvA" target="_blank" rel="noopener">https://pan.baidu.com/s/1cYtc_aDyDFogjTl47VyfvA</a>  密码:sz34</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/jiangmen/jiangmen.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;

&lt;p&gt;将门社区技术分享，&lt;a href=&quot;https://www.techbeat.net/talk-info?id=384&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;techbeat官网链接&lt;/a&gt; / &lt;a href=&quot;https://www.bilibili.com/video/BV1qk4y1z7cV&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;bilibili直播链接&lt;/a&gt;。&lt;br&gt;ppt文件分享，&lt;a href=&quot;https://pan.baidu.com/s/1cYtc_aDyDFogjTl47VyfvA&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Baidu Cloud&lt;/a&gt;&lt;br&gt;链接:&lt;a href=&quot;https://pan.baidu.com/s/1cYtc_aDyDFogjTl47VyfvA&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://pan.baidu.com/s/1cYtc_aDyDFogjTl47VyfvA&lt;/a&gt;  密码:sz34&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>One tech talk</title>
    <link href="http://tangxuvis.github.io/2020/04/25/One-tech-talk/"/>
    <id>http://tangxuvis.github.io/2020/04/25/One-tech-talk/</id>
    <published>2020-04-25T15:19:43.000Z</published>
    <updated>2020-04-27T09:04:06.400Z</updated>
    
    <content type="html"><![CDATA[<p>Someone share video of my talk on youtube, please see details here.<br><a href="https://www.youtube.com/watch?reload=9&amp;v=kA9FWQjjU_4&amp;list=PLiG8_90geV" target="_blank" rel="noopener">https://www.youtube.com/watch?reload=9&amp;v=kA9FWQjjU_4&amp;list=PLiG8_90geV</a><br><a href="https://www.youtube.com/watch?v=FXC0b9yNOX0" target="_blank" rel="noopener">https://www.youtube.com/watch?v=FXC0b9yNOX0</a></p><p align="center"><img src="/imgs/iccv2019_workshop_ppt/talk_video.jpg" hspace="10"> <br></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Someone share video of my talk on youtube, please see details here.&lt;br&gt;&lt;a href=&quot;https://www.youtube.com/watch?reload=9&amp;amp;v=kA9FWQjjU_4&amp;amp;list=PLiG8_90geV&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.youtube.com/watch?reload=9&amp;amp;v=kA9FWQjjU_4&amp;amp;list=PLiG8_90geV&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=FXC0b9yNOX0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.youtube.com/watch?v=FXC0b9yNOX0&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/iccv2019_workshop_ppt/talk_video.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>人脸检测技术发展及百度“框”实践</title>
    <link href="http://tangxuvis.github.io/2020/04/21/history_of_face_detection/"/>
    <id>http://tangxuvis.github.io/2020/04/21/history_of_face_detection/</id>
    <published>2020-04-21T08:34:41.000Z</published>
    <updated>2020-04-21T08:39:29.518Z</updated>
    
    <content type="html"><![CDATA[<p>撰文<a href="https://www.jiqizhixin.com/articles/2020-04-17-4?from=timeline&isappinstalled=0" target="_blank" rel="noopener">《人脸检测技术发展及百度“框”实践》</a>，发表在机器之心栏目。</p><p align="center"><img src="/imgs/cvpr2020/history.jpg" height="400" width="600" hspace="10"> <br></p><p align="center"><img src="/imgs/cvpr2020/introduction.jpg" height="400" width="600" hspace="10"> <br></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;撰文&lt;a href=&quot;https://www.jiqizhixin.com/articles/2020-04-17-4?from=timeline&amp;isappinstalled=0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《人脸检测技术发展及百度“框”实践》&lt;/a&gt;，发表在机器之心栏目。&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/cvpr2020/history.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/cvpr2020/introduction.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>News:2 papers accepted by CVPR2020</title>
    <link href="http://tangxuvis.github.io/2020/04/21/cvpr2020_hambox_bfbox/"/>
    <id>http://tangxuvis.github.io/2020/04/21/cvpr2020_hambox_bfbox/</id>
    <published>2020-04-21T08:25:04.000Z</published>
    <updated>2020-04-21T08:33:56.725Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1912.09231" target="_blank" rel="noopener">HAMBox: Delving into Online High-quality Anchors Mining for Detecting Outer Faces</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Current face detectors utilize anchors to frame a multi-task learning problem which combines classification and bounding box regression. Effective anchor design and anchor matching strategy enable face detectors to localize faces under large pose and scale variations. However, we observe that more than 80% correctly predicted bounding boxes are regressed from the unmatched anchors (the IoUs between anchors and target faces are lower than a threshold) in the inference phase. It indicates that these unmatched anchors perform excellent regression ability, but the existing methods neglect to learn from them. In this paper, we propose an Online High-quality Anchor Mining Strategy (HAMBox), which explicitly helps outer faces compensate with high-quality anchors. Our proposed HAMBox method could be a general strategy for anchor-based single-stage face detection. Experiments on various datasets, including WIDER FACE, FDDB, AFW and PASCAL Face, demonstrate the superiority of the proposed method. Furthermore, our team win the championship on the Face Detection test track of WIDER Face and Pedestrian Challenge 2019. We will release the codes with PaddlePaddle.</p><p align="center"><img src="/imgs/cvpr2020/cvpr2020_hambox.jpg" height="400" width="600" hspace="10"> <br></p><p>BFBox: Searching Face-appropriate Backbone and Feature Pyramid Network for Robust Face Detector</p><h2 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文提出的方法BFBox是基于神经网络架构搜索（NAS）的方法同时搜索适合人脸检测的特征提取器和特征金字塔。动机是我们发现了一个有趣的现象：针对图像分类任务设计的流行的特征提取器已经在通用目标检测任务上验证了其重要的兼容性，然而在人脸检测任务上却没有取得预期的效果。同时不同的特征提取器与特征金字塔的结合也不是完全正相关的。首先，本文对于比较好的特征提取器进行分析，提出了适合人脸的搜索空间；其次，提出了图1的特征金字塔注意力模块（FPN-attention Module）去加强特征提取器和特征金字塔之间的联系；最后, 采取SNAS的方法同时搜出适和人脸的特征提取器和特征金字塔结构。多个数据集上（WIDER FACE, FDDB, AFW和PASCAL Face）的实验表明了我们提出的方法的优越性。<br>如下图所示为检测网络的结构。网络是基于RetinaNet的结构加上我们提出的特征金字塔注意力模块（FPN-attention Module），训练超网络时采用的是随机采样的方法。</p><p align="center"><img src="/imgs/cvpr2020/cvpr2020_bfbox.jpg" height="400" width="600" hspace="10"> <br></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.09231&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HAMBox: Delving into Online High-quality Anchors Mining for Detecting Outer Faces&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;Current face detectors utilize anchors to frame a multi-task learning problem which combines classification and bounding box regression. Effective anchor design and anchor matching strategy enable face detectors to localize faces under large pose and scale variations. However, we observe that more than 80% correctly predicted bounding boxes are regressed from the unmatched anchors (the IoUs between anchors and target faces are lower than a threshold) in the inference phase. It indicates that these unmatched anchors perform excellent regression ability, but the existing methods neglect to learn from them. In this paper, we propose an Online High-quality Anchor Mining Strategy (HAMBox), which explicitly helps outer faces compensate with high-quality anchors. Our proposed HAMBox method could be a general strategy for anchor-based single-stage face detection. Experiments on various datasets, including WIDER FACE, FDDB, AFW and PASCAL Face, demonstrate the superiority of the proposed method. Furthermore, our team win the championship on the Face Detection test track of WIDER Face and Pedestrian Challenge 2019. We will release the codes with PaddlePaddle.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/cvpr2020/cvpr2020_hambox.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;


&lt;p&gt;BFBox: Searching Face-appropriate Backbone and Feature Pyramid Network for Robust Face Detector&lt;/p&gt;
&lt;h2 id=&quot;Abstract-1&quot;&gt;&lt;a href=&quot;#Abstract-1&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;本文提出的方法BFBox是基于神经网络架构搜索（NAS）的方法同时搜索适合人脸检测的特征提取器和特征金字塔。动机是我们发现了一个有趣的现象：针对图像分类任务设计的流行的特征提取器已经在通用目标检测任务上验证了其重要的兼容性，然而在人脸检测任务上却没有取得预期的效果。同时不同的特征提取器与特征金字塔的结合也不是完全正相关的。首先，本文对于比较好的特征提取器进行分析，提出了适合人脸的搜索空间；其次，提出了图1的特征金字塔注意力模块（FPN-attention Module）去加强特征提取器和特征金字塔之间的联系；最后, 采取SNAS的方法同时搜出适和人脸的特征提取器和特征金字塔结构。多个数据集上（WIDER FACE, FDDB, AFW和PASCAL Face）的实验表明了我们提出的方法的优越性。&lt;br&gt;如下图所示为检测网络的结构。网络是基于RetinaNet的结构加上我们提出的特征金字塔注意力模块（FPN-attention Module），训练超网络时采用的是随机采样的方法。&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/cvpr2020/cvpr2020_bfbox.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Our Open Source Projects</title>
    <link href="http://tangxuvis.github.io/2020/04/21/open_source_projects/"/>
    <id>http://tangxuvis.github.io/2020/04/21/open_source_projects/</id>
    <published>2020-04-21T08:06:31.000Z</published>
    <updated>2020-04-21T08:23:32.060Z</updated>
    
    <content type="html"><![CDATA[<p>基于身份保持的条件对抗生成网络的人脸老化IPCGAN (CVPR2018)<br><a href="https://github.com/dawei6875797/Face-Aging-with-Identity-Preserved-Conditional-Generative-Adversarial-Networks" target="_blank" rel="noopener">https://github.com/dawei6875797/Face-Aging-with-Identity-Preserved-Conditional-Generative-Adversarial-Networks</a><br>PyramidBox人脸检测器 (ECCV2018)<br><a href="https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/face_detection" target="_blank" rel="noopener">https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/face_detection</a><br>人脸检测轻量化模型faceboxes和blazeface<br><a href="https://github.com/PaddlePaddle/PaddleDetection/tree/release/0.2/configs/face_detection" target="_blank" rel="noopener">https://github.com/PaddlePaddle/PaddleDetection/tree/release/0.2/configs/face_detection</a><br>[抗击肺炎] 口罩人脸检测与分类<br><a href="https://www.paddlepaddle.org.cn/hub/scene/maskdetect" target="_blank" rel="noopener">https://www.paddlepaddle.org.cn/hub/scene/maskdetect</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;基于身份保持的条件对抗生成网络的人脸老化IPCGAN (CVPR2018)&lt;br&gt;&lt;a href=&quot;https://github.com/dawei6875797/Face-Aging-with-Identity-Preserved-Conditional-Generati
      
    
    </summary>
    
    
    
      <category term="Publications" scheme="http://tangxuvis.github.io/tags/Publications/"/>
    
  </entry>
  
  <entry>
    <title>Share our PPT about &#39;Delveing into High Performance Detector for Finding Tiny Faces&#39; on ICCV</title>
    <link href="http://tangxuvis.github.io/2019/11/04/iccv2019-workshop-ppt/"/>
    <id>http://tangxuvis.github.io/2019/11/04/iccv2019-workshop-ppt/</id>
    <published>2019-11-04T05:54:11.000Z</published>
    <updated>2019-11-04T06:21:16.105Z</updated>
    
    <content type="html"><![CDATA[<p>On 11.02.2019, we give a presentation on <a href="https://frchallenge.github.io/index" target="_blank" rel="noopener">ICCV 2019 Workshop<br>Face Recognition in the Wild</a>, and this is the slides.<br>Link:<br><a href="https://pan.baidu.com/s/13VuTE7U3tL2OzuNGsmaYJg" target="_blank" rel="noopener">Baidu Cloud</a><br><a href="https://drive.google.com/open?id=1fu8r2hs96ChrhlPDHeVzQneVH1zd-2m8" target="_blank" rel="noopener">Google Drive</a></p><p align="center"><img src="/imgs/iccv2019_workshop_ppt/ppt.jpg" hspace="10"> <br></p><a id="more"></a><p align="center"><img src="/imgs/iccv2019_workshop_ppt/workshop_begin.jpg" hspace="10"> <br></p><p align="center"><img src="/imgs/iccv2019_workshop_ppt/talk.jpg" hspace="10"> <br></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;On 11.02.2019, we give a presentation on &lt;a href=&quot;https://frchallenge.github.io/index&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ICCV 2019 Workshop&lt;br&gt;Face Recognition in the Wild&lt;/a&gt;, and this is the slides.&lt;br&gt;Link:&lt;br&gt;&lt;a href=&quot;https://pan.baidu.com/s/13VuTE7U3tL2OzuNGsmaYJg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Baidu Cloud&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://drive.google.com/open?id=1fu8r2hs96ChrhlPDHeVzQneVH1zd-2m8&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Google Drive&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/iccv2019_workshop_ppt/ppt.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>News:Our paper and code will be released in this page.</title>
    <link href="http://tangxuvis.github.io/2019/10/27/wider_challenge_2019/"/>
    <id>http://tangxuvis.github.io/2019/10/27/wider_challenge_2019/</id>
    <published>2019-10-27T04:36:59.000Z</published>
    <updated>2019-10-27T05:08:10.120Z</updated>
    
    <content type="html"><![CDATA[<p>1st place and 1 invited talk in <a href="https://wider-challenge.org/2019.html" target="_blank" rel="noopener">face detection track on ICCV Wider Challenge 2019</a>.<br>More details, including tech report and code, will be introduced in this page.</p><p>TBD …</p><p align="center"><img src="/imgs/iccv2019_wider_challenge/leaderboard.jpg" hspace="10"> <br>ICCV Wider Challenge优胜方案</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1st place and 1 invited talk in &lt;a href=&quot;https://wider-challenge.org/2019.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;face detection track on ICCV Wider Challenge 2019&lt;/a&gt;.&lt;br&gt;More details, including tech report and code, will be introduced in this page.&lt;/p&gt;
&lt;p&gt;TBD …&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/iccv2019_wider_challenge/leaderboard.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
ICCV Wider Challenge优胜方案
&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Challenge" scheme="http://tangxuvis.github.io/tags/Challenge/"/>
    
  </entry>
  
  <entry>
    <title>News:1 paper accepted by TIFS2019</title>
    <link href="http://tangxuvis.github.io/2019/09/04/tifs2019_daf/"/>
    <id>http://tangxuvis.github.io/2019/09/04/tifs2019_daf/</id>
    <published>2019-09-04T15:36:59.000Z</published>
    <updated>2019-10-05T18:28:46.148Z</updated>
    
    <content type="html"><![CDATA[<p>1 paper <a href="https://ieeexplore.ieee.org/document/8839888" target="_blank" rel="noopener">Progressively Refined Face Detection Through Semantics-Enriched Representation Learning</a> accepted by <a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-information-forensics-and-security" target="_blank" rel="noopener">IEEE Transactions on Information Forensics and Security (TIFS)</a> — CCF A.</p><p align="center"><img src="/imgs/tifs2019_daf/architecture.jpg" hspace="10"> <br></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 paper &lt;a href=&quot;https://ieeexplore.ieee.org/document/8839888&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Progressively Refined Face Detection Through Semantics-Enriched Representation Learning&lt;/a&gt; accepted by &lt;a href=&quot;https://signalprocessingsociety.org/publications-resources/ieee-transactions-information-forensics-and-security&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;IEEE Transactions on Information Forensics and Security (TIFS)&lt;/a&gt; — CCF A.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/tifs2019_daf/architecture.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Publications" scheme="http://tangxuvis.github.io/tags/Publications/"/>
    
  </entry>
  
  <entry>
    <title>News:1st place and 1 invited talk in face detection track on ICCV Wider Challenge 2019</title>
    <link href="http://tangxuvis.github.io/2019/08/04/iccv2019_wider_challenge/"/>
    <id>http://tangxuvis.github.io/2019/08/04/iccv2019_wider_challenge/</id>
    <published>2019-08-04T15:36:59.000Z</published>
    <updated>2020-04-21T08:21:57.351Z</updated>
    
    <content type="html"><![CDATA[<p>1st place and 1 invited talk in <a href="https://wider-challenge.org/2019.html" target="_blank" rel="noopener">face detection track on ICCV Wider Challenge 2019</a>. See you in Korea.</p><p align="center"><img src="/imgs/iccv2019_wider_challenge/leaderboard.jpg" hspace="10"> <br>ICCV Wider Challenge优胜方案</p><p>This is the slides.<br><a href="https://pan.baidu.com/s/1QKUp4NZGK0nBZWzUqhBoVA" target="_blank" rel="noopener">Baidu Cloud</a><br>链接:<a href="https://pan.baidu.com/s/1QKUp4NZGK0nBZWzUqhBoVA" target="_blank" rel="noopener">https://pan.baidu.com/s/1QKUp4NZGK0nBZWzUqhBoVA</a>  密码:1hbx</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1st place and 1 invited talk in &lt;a href=&quot;https://wider-challenge.org/2019.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;face detection track on ICCV Wider Challenge 2019&lt;/a&gt;. See you in Korea.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/iccv2019_wider_challenge/leaderboard.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
ICCV Wider Challenge优胜方案
&lt;/p&gt;

&lt;p&gt;This is the slides.&lt;br&gt;&lt;a href=&quot;https://pan.baidu.com/s/1QKUp4NZGK0nBZWzUqhBoVA&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Baidu Cloud&lt;/a&gt;&lt;br&gt;链接:&lt;a href=&quot;https://pan.baidu.com/s/1QKUp4NZGK0nBZWzUqhBoVA&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://pan.baidu.com/s/1QKUp4NZGK0nBZWzUqhBoVA&lt;/a&gt;  密码:1hbx&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Challenge" scheme="http://tangxuvis.github.io/tags/Challenge/"/>
    
  </entry>
  
  <entry>
    <title>News:1st place and 1 invited talk in VOT-ST2019</title>
    <link href="http://tangxuvis.github.io/2019/07/04/iccv2019_vot/"/>
    <id>http://tangxuvis.github.io/2019/07/04/iccv2019_vot/</id>
    <published>2019-07-04T15:36:59.000Z</published>
    <updated>2020-04-21T08:22:02.933Z</updated>
    
    <content type="html"><![CDATA[<p>1st place and 1 invited talk in <a href="http://www.votchallenge.net/vot2019/index.html" target="_blank" rel="noopener">VOT-ST2019</a>. See you in Korea.</p><p align="center"><img src="/imgs/iccv2019_vot/logo.jpg" hspace="10"> <br>vot优胜方案</p><p>This is the slides.<br><a href="https://pan.baidu.com/s/1TghiaHtvbroNkRRaSlOEpg" target="_blank" rel="noopener">Baidu Cloud</a><br>链接:<a href="https://pan.baidu.com/s/1TghiaHtvbroNkRRaSlOEpg" target="_blank" rel="noopener">https://pan.baidu.com/s/1TghiaHtvbroNkRRaSlOEpg</a>  密码:6x0c</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1st place and 1 invited talk in &lt;a href=&quot;http://www.votchallenge.net/vot2019/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;VOT-ST2019&lt;/a&gt;. See you in Korea.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/iccv2019_vot/logo.jpg&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
vot优胜方案
&lt;/p&gt;

&lt;p&gt;This is the slides.&lt;br&gt;&lt;a href=&quot;https://pan.baidu.com/s/1TghiaHtvbroNkRRaSlOEpg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Baidu Cloud&lt;/a&gt;&lt;br&gt;链接:&lt;a href=&quot;https://pan.baidu.com/s/1TghiaHtvbroNkRRaSlOEpg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://pan.baidu.com/s/1TghiaHtvbroNkRRaSlOEpg&lt;/a&gt;  密码:6x0c&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Challenge" scheme="http://tangxuvis.github.io/tags/Challenge/"/>
    
  </entry>
  
  <entry>
    <title>News:1 paper submited to arxiv</title>
    <link href="http://tangxuvis.github.io/2019/04/04/arxiv2019_dubox/"/>
    <id>http://tangxuvis.github.io/2019/04/04/arxiv2019_dubox/</id>
    <published>2019-04-04T15:36:59.000Z</published>
    <updated>2019-10-05T18:25:12.423Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1904.06883" target="_blank" rel="noopener">DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Traditional neural objection detection methods use multi-scale features that allow multiple detectors to perform<br>detecting tasks independently and in parallel. At the same time, with the handling of the prior box, the algorithm’s<br>ability to deal with scale invariance is enhanced. However, too many prior boxes and independent detectors will<br>increase the computational redundancy of the detection algorithm. In this study, we introduce Dubox, a new one-stage<br>approach that detects the objects without prior box. Working with multi-scale features, the designed dual scale residual unit makes dual scale detectors no longer run independently. The second scale detector learns the residual of the first. Dubox has enhanced the capacity of heuristic-guided that can further enable the first scale detector to maximize the detection of small targets and the second to detect objects that cannot be identified by the first one. Besides, for each scale detector, with the new classification-regression<br>progressive strapped loss makes our process not based on prior boxes. Integrating these strategies, our detection algorithm has achieved excellent performance in terms of speed and accuracy. Extensive experiments on the VOC, COCO object detection benchmark have confirmed the effectiveness of this algorithm.</p><p align="center"><img src="/imgs/arxiv2019_dubox/architecture.jpg" height="400" width="600" hspace="10"> <br></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.06883&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DuBox: No-Prior Box Objection Detection via Residual Dual Scale Detectors&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;Traditional neural objection detection methods use multi-scale features that allow multiple detectors to perform&lt;br&gt;detecting tasks independently and in parallel. At the same time, with the handling of the prior box, the algorithm’s&lt;br&gt;ability to deal with scale invariance is enhanced. However, too many prior boxes and independent detectors will&lt;br&gt;increase the computational redundancy of the detection algorithm. In this study, we introduce Dubox, a new one-stage&lt;br&gt;approach that detects the objects without prior box. Working with multi-scale features, the designed dual scale residual unit makes dual scale detectors no longer run independently. The second scale detector learns the residual of the first. Dubox has enhanced the capacity of heuristic-guided that can further enable the first scale detector to maximize the detection of small targets and the second to detect objects that cannot be identified by the first one. Besides, for each scale detector, with the new classification-regression&lt;br&gt;progressive strapped loss makes our process not based on prior boxes. Integrating these strategies, our detection algorithm has achieved excellent performance in terms of speed and accuracy. Extensive experiments on the VOC, COCO object detection benchmark have confirmed the effectiveness of this algorithm.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;/imgs/arxiv2019_dubox/architecture.jpg&quot; height=&quot;400&quot; width=&quot;600&quot; hspace=&quot;10&quot;&gt; &lt;br&gt;
&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Publications" scheme="http://tangxuvis.github.io/tags/Publications/"/>
    
  </entry>
  
</feed>
